{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas**  \n",
    "When there is a CSV data, efficient and effective way to handle is to use pandas  \n",
    "This can be used for various data handlind and manipulation  \n",
    "That makes a good choice for Data Retirieval mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv ('Student_Performance.csv')\n",
    "print (Data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Check pandas functions that can be used for data calculations and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various data computation possible\n",
    "print ('Number of records : ', len (Data))\n",
    "print ('Min, Avg, Max Study Hours : ', Data['study_hours_per_day'].min(), Data['study_hours_per_day'].mean(), Data['study_hours_per_day'].max())\n",
    "print (\"Values in Paretal Education :\", Data['parental_education_level'].unique())\n",
    "print (\"Values in Internet Quality :\", Data['internet_quality'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering**  \n",
    "Pandas provides conditional filter / slicing functions that can be used as part of retrieval  \n",
    "Result again in another pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for poor internet quality and score more than 90\n",
    "# All columns considered\n",
    "Filtered = Data[(Data['internet_quality'] =='Poor') & (Data['exam_score'] > 90.0)]\n",
    "Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for above average study hours and attendance, but low marks\n",
    "Filtered = Data[(Data['study_hours_per_day'] >= Data['study_hours_per_day'].mean()) & \n",
    "                (Data['attendance_percentage'] >= Data['attendance_percentage'].mean()) & \n",
    "                (Data['exam_score'] < 60)]\n",
    "Filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use as Context**  \n",
    "The data filtered using pandas used as context to provide LLM the data  \n",
    "This data retrieval can be quick and effective  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = Groq()\n",
    "# client = Groq(api_key=\"Your Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Specific instructions in terms of Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Instr = \"Using the context given, provide response to the user question or statement.\\\n",
    "            Context is provided as CSV formatted string.\\\n",
    "            Answer to the question with details\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt\n",
    "Prompt = \"Why do you think students score low marks despite attending class and studying well?\"\n",
    "\n",
    "Filtered = Data[(Data['study_hours_per_day'] >= Data['study_hours_per_day'].mean()) & \n",
    "                (Data['attendance_percentage'] >= Data['attendance_percentage'].mean()) & \n",
    "                (Data['exam_score'] < 60)]\n",
    "Context = Filtered.to_csv (index=False, float_format='%.1f')\n",
    "\n",
    "# Invoke LLM with prompt and context\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": R_Instr\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":\"Context : \\n\"+ Context + \"Query : \\n\" + Prompt\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "\n",
    "print (completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Let's try adapting the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Instr = \"Using the context given, provide response to the user question or statement.\\\n",
    "            Context is provided as CSV formatted string.\\\n",
    "            Provide comprehensive response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt\n",
    "Prompt = \"Students who have good sleep patterns, are they utlising the time properly?\"\n",
    "\n",
    "# Filter relevant rows for above average sleep time and >75 mark. Only relevant columns\n",
    "Filtered = Data.loc[(Data['sleep_hours'] >= Data['sleep_hours'].mean()*1.1) & \n",
    "                (Data['exam_score'] >= 75),\n",
    "                ['study_hours_per_day', 'social_media_hours', 'netflix_hours', 'part_time_job', 'attendance_percentage']]\n",
    "Context = Filtered.to_csv (index=False, float_format='%.1f')\n",
    "\n",
    "# Invoke LLM with prompt and context\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": R_Instr\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":\"Context : \\n\"+ Context + \"Query : \\n\" + Prompt\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "\n",
    "print (completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Code from LLM**  \n",
    "Like in SQL, in pandas also we can take help of LLM to generate code  \n",
    "This would require specific instructions to be provided for code generation  \n",
    "The generated code can be used to extract data from dataframe and then use it as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_Instr = \"For the given pandas dtypes, write code lines that can filter out data to answer user question.\\\n",
    "            Study the pandas dtypes and user question clearly\\\n",
    "            Give only the python code lines for necessary filtering of the Dataframe variable.\\\n",
    "            Code that can filter and return dataframe. Assign results to 'Result'\\\n",
    "            No additional code / string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke LLM with prompt and context\n",
    "Prompt = \"Who all have good score despite having attendance below 75?\"\n",
    "# Prompt = \"how many scored > 90% with less than 70% attendance?\"\n",
    "# Prompt = \"What are the students who score good despite of low study time do?\"\n",
    "\n",
    "Data_Frame_Name = \"Data\"\n",
    "\n",
    "Dtype = str(Data.dtypes)\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": C_Instr\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Data.dtype : \\n\"+ Dtype + \"Dataframe variable : 'Data'\\n Question : \\n\" + Prompt\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "\n",
    "Code = completion.choices[0].message.content\n",
    "Code_Clean = Code.strip (\"`\")\n",
    "print (Code_Clean)\n",
    "\n",
    "exec (Code_Clean)\n",
    "# print (Result)\n",
    "\n",
    "Context = Result.to_csv (index=False, float_format='%.1f')\n",
    "\n",
    "# Invoke LLM with prompt and context\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": R_Instr\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":\"Context : \\n\"+ Context + \"Query : \\n\" + Prompt\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "\n",
    "print (completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas SQL**  \n",
    "using the pandasql library, SQL query can be raised on data frame (treatig like a table)  \n",
    "The psql library can handle basic SQL queries which are typically handled by SQLite  \n",
    "Since it can be used as SQL queries, it can be integrated in RAG pipe line (query by LLM etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = \"SELECT * FROM Data WHERE extracurricular_participation = 'Yes'\"\n",
    "\n",
    "result = psql.sqldf(Query)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tab_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
