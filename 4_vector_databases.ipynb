{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Vector Databases\n",
    "\n",
    "This notebook demonstrates how to use vector databases for efficient semantic search and retrieval.\n",
    "\n",
    "## What is a Vector Database?\n",
    "\n",
    "A **vector database** is a specialized database designed to store, index, and search high-dimensional vectors (embeddings) efficiently.\n",
    "\n",
    "### Why Do We Need Vector Databases?\n",
    "\n",
    "Traditional databases (SQL, NoSQL) are great for exact matches:\n",
    "- `WHERE name = 'John'`\n",
    "- `WHERE price > 100`\n",
    "\n",
    "But they struggle with:\n",
    "- Finding similar meanings: \"laptop\" vs \"notebook computer\"\n",
    "- Semantic search: \"affordable portable computers\" ‚Üí find laptops\n",
    "- Nearest neighbor search in high-dimensional space\n",
    "\n",
    "### Vector Database Capabilities:\n",
    "\n",
    "1. **Efficient Storage**: Store millions of high-dimensional vectors\n",
    "2. **Fast Similarity Search**: Find nearest neighbors in milliseconds\n",
    "3. **Metadata Filtering**: Combine semantic search with traditional filters\n",
    "4. **Scalability**: Handle large-scale applications\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **RAG (Retrieval Augmented Generation)**: Find relevant context for LLMs\n",
    "- **Semantic Search**: Search by meaning, not keywords\n",
    "- **Recommendation Systems**: Find similar items\n",
    "- **Duplicate Detection**: Find similar documents\n",
    "- **Question Answering**: Match questions to answers\n",
    "\n",
    "---\n",
    "\n",
    "## Vector Databases We'll Explore\n",
    "\n",
    "### 1. FAISS (Facebook AI Similarity Search)\n",
    "- **Type**: In-memory vector search library\n",
    "- **Best For**: Fast similarity search, research, prototyping\n",
    "- **Pros**: Extremely fast, battle-tested, many index types\n",
    "- **Cons**: In-memory only, no native persistence features\n",
    "\n",
    "### 2. LanceDB\n",
    "- **Type**: Embedded vector database\n",
    "- **Best For**: Production applications, persistent storage\n",
    "- **Pros**: Disk-based, SQL-like queries, versioning, cloud-native\n",
    "- **Cons**: Newer, smaller community\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "! pip install faiss-cpu lancedb sentence-transformers pypdf python-dotenv mistralai pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PDF processing\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Embeddings\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Vector databases\n",
    "import faiss\n",
    "import lancedb\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: PDF Parsing and Chunking\n",
    "\n",
    "First, we need to extract text from PDFs and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PDF parsing and chunking functions defined\n"
     ]
    }
   ],
   "source": [
    "def parse_pdf(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse PDF and extract text with metadata.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with page text and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìÑ Parsing PDF: {pdf_path}\")\n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages_data = []\n",
    "    \n",
    "    for page_num, page in enumerate(reader.pages, start=1):\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        if text.strip():  # Only add if there's actual text\n",
    "            pages_data.append({\n",
    "                'page_number': page_num,\n",
    "                'text': text,\n",
    "                'file_name': Path(pdf_path).name\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úì Extracted text from {len(pages_data)} pages\")\n",
    "    return pages_data\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks with sentence boundary awareness.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_len = len(text)\n",
    "    \n",
    "    while start < text_len:\n",
    "        # Define the end of this chunk\n",
    "        end = min(start + chunk_size, text_len)\n",
    "        \n",
    "        # Extract chunk\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Try to break at sentence boundary (only if not at the very end)\n",
    "        if end < text_len:\n",
    "            # Look for sentence boundaries\n",
    "            last_period = chunk.rfind('.')\n",
    "            last_newline = chunk.rfind('\\n')\n",
    "            last_question = chunk.rfind('?')\n",
    "            last_exclamation = chunk.rfind('!')\n",
    "            \n",
    "            break_point = max(last_period, last_newline, last_question, last_exclamation)\n",
    "            \n",
    "            # Only use the break point if it's reasonably far into the chunk\n",
    "            if break_point > chunk_size * 0.5:\n",
    "                chunk = chunk[:break_point + 1]\n",
    "        \n",
    "        # Add chunk if it has content\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk.strip())\n",
    "        \n",
    "        # Move start position forward by (chunk_size - overlap)\n",
    "        # This guarantees we make progress even with short adjusted chunks\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_pdf_to_chunks(pdf_path: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse PDF and split into chunks with metadata.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with chunks and metadata\n",
    "    \"\"\"\n",
    "    pages_data = parse_pdf(pdf_path)\n",
    "    all_chunks = []\n",
    "    chunk_id = 0\n",
    "    \n",
    "    for page_data in pages_data:\n",
    "        chunks = chunk_text(page_data['text'], chunk_size, overlap)\n",
    "        \n",
    "        for chunk_num, chunk in enumerate(chunks, start=1):\n",
    "            all_chunks.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'text': chunk,\n",
    "                'file_name': page_data['file_name'],\n",
    "                'page_number': page_data['page_number'],\n",
    "                'chunk_number': chunk_num,\n",
    "                'char_count': len(chunk)\n",
    "            })\n",
    "            chunk_id += 1\n",
    "    \n",
    "    print(f\"‚úì Created {len(all_chunks)} chunks from {len(pages_data)} pages\")\n",
    "    return all_chunks\n",
    "\n",
    "print(\"‚úì PDF parsing and chunking functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample PDF for Testing\n",
    "\n",
    "Let's create a sample PDF with technical content for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Sample content created\n",
      "Content length: 4082 characters\n",
      "\n",
      "First 200 characters:\n",
      "Machine Learning Fundamentals\n",
      "\n",
      "Introduction to Machine Learning\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicit...\n"
     ]
    }
   ],
   "source": [
    "# Create sample content\n",
    "sample_content = \"\"\"Machine Learning Fundamentals\n",
    "\n",
    "Introduction to Machine Learning\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
    "\n",
    "Supervised Learning\n",
    "Supervised learning is a type of machine learning where the algorithm learns from labeled training data. The algorithm makes predictions based on input data and is corrected when its predictions are incorrect. Common applications include classification and regression tasks.\n",
    "\n",
    "Classification algorithms predict discrete labels. For example, determining whether an email is spam or not spam. Popular classification algorithms include logistic regression, decision trees, random forests, and support vector machines.\n",
    "\n",
    "Regression algorithms predict continuous values. For instance, predicting house prices based on features like size, location, and age. Linear regression and polynomial regression are fundamental regression techniques.\n",
    "\n",
    "Unsupervised Learning\n",
    "Unsupervised learning involves training algorithms on unlabeled data. The system tries to learn the patterns and structure from the data without explicit guidance. Clustering and dimensionality reduction are primary unsupervised learning techniques.\n",
    "\n",
    "K-means clustering groups similar data points together. It's widely used in customer segmentation, image compression, and anomaly detection. The algorithm iteratively assigns data points to clusters based on feature similarity.\n",
    "\n",
    "Deep Learning\n",
    "Deep learning uses artificial neural networks with multiple layers to progressively extract higher-level features from raw input. It has revolutionized fields like computer vision, natural language processing, and speech recognition.\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are particularly effective for image processing tasks. They use convolutional layers to automatically learn spatial hierarchies of features, making them ideal for tasks like image classification and object detection.\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed for sequence data. They maintain an internal state that captures information about previous inputs, making them suitable for tasks like language modeling and time series prediction.\n",
    "\n",
    "Natural Language Processing\n",
    "Natural Language Processing (NLP) focuses on the interaction between computers and human language. Modern NLP uses transformer architectures like BERT and GPT, which have achieved state-of-the-art results across various language tasks.\n",
    "\n",
    "Word embeddings represent words as dense vectors in a continuous vector space, where semantically similar words are closer together. Popular embedding techniques include Word2Vec, GloVe, and contextual embeddings from transformer models.\n",
    "\n",
    "Model Evaluation\n",
    "Evaluating machine learning models is crucial for understanding their performance. Common metrics include accuracy, precision, recall, F1-score for classification, and mean squared error, R-squared for regression.\n",
    "\n",
    "Cross-validation helps assess how well a model generalizes to unseen data. K-fold cross-validation divides the data into k subsets and trains the model k times, each time using a different subset for validation.\n",
    "\n",
    "Overfitting and Underfitting\n",
    "Overfitting occurs when a model learns the training data too well, including its noise, resulting in poor performance on new data. Regularization techniques like L1 and L2 regularization help prevent overfitting.\n",
    "\n",
    "Underfitting happens when a model is too simple to capture the underlying patterns in the data. This can be addressed by increasing model complexity or using more relevant features.\n",
    "\n",
    "Feature Engineering\n",
    "Feature engineering is the process of using domain knowledge to create features that make machine learning algorithms work better. Good features can significantly improve model performance.\n",
    "\n",
    "Feature scaling ensures that all features contribute equally to the model. Standardization and normalization are common scaling techniques that transform features to a similar scale.\n",
    "\"\"\"\n",
    "\n",
    "# Write to a text file (simulating PDF content)\n",
    "os.makedirs('sample_data', exist_ok=True)\n",
    "with open('sample_data/ml_fundamentals.txt', 'w') as f:\n",
    "    f.write(sample_content)\n",
    "\n",
    "print(\"‚úì Sample content created\")\n",
    "print(f\"Content length: {len(sample_content)} characters\")\n",
    "print(f\"\\nFirst 200 characters:\\n{sample_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROCESSING SAMPLE DOCUMENT\n",
      "================================================================================\n",
      "\n",
      "‚úì Extracted 18 sections\n"
     ]
    }
   ],
   "source": [
    "# For this demo, we'll work with the text file\n",
    "# In practice, you would use actual PDF files\n",
    "\n",
    "def parse_text_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Parse text file as if it were a PDF.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split into \"pages\" by double newlines (paragraph breaks)\n",
    "    sections = content.split('\\n\\n')\n",
    "    \n",
    "    pages_data = []\n",
    "    for page_num, section in enumerate(sections, start=1):\n",
    "        if section.strip():\n",
    "            pages_data.append({\n",
    "                'page_number': page_num,\n",
    "                'text': section,\n",
    "                'file_name': Path(file_path).name\n",
    "            })\n",
    "    \n",
    "    return pages_data\n",
    "\n",
    "# Process the sample file\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCESSING SAMPLE DOCUMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "file_path = 'sample_data/ml_fundamentals.txt'\n",
    "pages_data = parse_text_file(file_path)\n",
    "\n",
    "print(f\"\\n‚úì Extracted {len(pages_data)} sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_number': 1, 'text': 'Machine Learning Fundamentals', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 2, 'text': 'Introduction to Machine Learning\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 3, 'text': 'Supervised Learning\\nSupervised learning is a type of machine learning where the algorithm learns from labeled training data. The algorithm makes predictions based on input data and is corrected when its predictions are incorrect. Common applications include classification and regression tasks.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 4, 'text': 'Classification algorithms predict discrete labels. For example, determining whether an email is spam or not spam. Popular classification algorithms include logistic regression, decision trees, random forests, and support vector machines.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 5, 'text': 'Regression algorithms predict continuous values. For instance, predicting house prices based on features like size, location, and age. Linear regression and polynomial regression are fundamental regression techniques.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 6, 'text': 'Unsupervised Learning\\nUnsupervised learning involves training algorithms on unlabeled data. The system tries to learn the patterns and structure from the data without explicit guidance. Clustering and dimensionality reduction are primary unsupervised learning techniques.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 7, 'text': \"K-means clustering groups similar data points together. It's widely used in customer segmentation, image compression, and anomaly detection. The algorithm iteratively assigns data points to clusters based on feature similarity.\", 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 8, 'text': 'Deep Learning\\nDeep learning uses artificial neural networks with multiple layers to progressively extract higher-level features from raw input. It has revolutionized fields like computer vision, natural language processing, and speech recognition.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 9, 'text': 'Convolutional Neural Networks (CNNs) are particularly effective for image processing tasks. They use convolutional layers to automatically learn spatial hierarchies of features, making them ideal for tasks like image classification and object detection.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 10, 'text': 'Recurrent Neural Networks (RNNs) are designed for sequence data. They maintain an internal state that captures information about previous inputs, making them suitable for tasks like language modeling and time series prediction.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 11, 'text': 'Natural Language Processing\\nNatural Language Processing (NLP) focuses on the interaction between computers and human language. Modern NLP uses transformer architectures like BERT and GPT, which have achieved state-of-the-art results across various language tasks.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 12, 'text': 'Word embeddings represent words as dense vectors in a continuous vector space, where semantically similar words are closer together. Popular embedding techniques include Word2Vec, GloVe, and contextual embeddings from transformer models.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 13, 'text': 'Model Evaluation\\nEvaluating machine learning models is crucial for understanding their performance. Common metrics include accuracy, precision, recall, F1-score for classification, and mean squared error, R-squared for regression.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 14, 'text': 'Cross-validation helps assess how well a model generalizes to unseen data. K-fold cross-validation divides the data into k subsets and trains the model k times, each time using a different subset for validation.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 15, 'text': 'Overfitting and Underfitting\\nOverfitting occurs when a model learns the training data too well, including its noise, resulting in poor performance on new data. Regularization techniques like L1 and L2 regularization help prevent overfitting.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 16, 'text': 'Underfitting happens when a model is too simple to capture the underlying patterns in the data. This can be addressed by increasing model complexity or using more relevant features.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 17, 'text': 'Feature Engineering\\nFeature engineering is the process of using domain knowledge to create features that make machine learning algorithms work better. Good features can significantly improve model performance.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 18, 'text': 'Feature scaling ensures that all features contribute equally to the model. Standardization and normalization are common scaling techniques that transform features to a similar scale.\\n', 'file_name': 'ml_fundamentals.txt'}\n",
      "‚úì Created 23 chunks\n",
      "\n",
      "================================================================================\n",
      "SAMPLE CHUNKS\n",
      "================================================================================\n",
      "\n",
      "Chunk 0:\n",
      "  File: ml_fundamentals.txt\n",
      "  Page: 1 | Chunk: 1\n",
      "  Length: 29 chars\n",
      "  Text: Machine Learning Fundamentals...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 1:\n",
      "  File: ml_fundamentals.txt\n",
      "  Page: 2 | Chunk: 1\n",
      "  Length: 291 chars\n",
      "  Text: Introduction to Machine Learning\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience wit...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "  File: ml_fundamentals.txt\n",
      "  Page: 2 | Chunk: 2\n",
      "  Length: 40 chars\n",
      "  Text: data and use it to learn for themselves....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create chunks\n",
    "all_chunks = []\n",
    "chunk_id = 0\n",
    "\n",
    "for page_data in pages_data:\n",
    "    print(page_data)\n",
    "    chunks = chunk_text(page_data['text'], chunk_size=300, overlap=50)\n",
    "    \n",
    "    for chunk_num, chunk in enumerate(chunks, start=1):\n",
    "        all_chunks.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'text': chunk,\n",
    "            'file_name': page_data['file_name'],\n",
    "            'page_number': page_data['page_number'],\n",
    "            'chunk_number': chunk_num,\n",
    "            'char_count': len(chunk)\n",
    "        })\n",
    "        chunk_id += 1\n",
    "\n",
    "print(f\"‚úì Created {len(all_chunks)} chunks\")\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE CHUNKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, chunk in enumerate(all_chunks[:3]):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  File: {chunk['file_name']}\")\n",
    "    print(f\"  Page: {chunk['page_number']} | Chunk: {chunk['chunk_number']}\")\n",
    "    print(f\"  Length: {chunk['char_count']} chars\")\n",
    "    print(f\"  Text: {chunk['text'][:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Parsing PDF: IndianBudget2025.pdf\n",
      "‚úì Extracted text from 58 pages\n",
      "‚úì Created 399 chunks from 58 pages\n"
     ]
    }
   ],
   "source": [
    "all_chunks = process_pdf_to_chunks('IndianBudget2025.pdf', chunk_size=300, overlap=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Generate Embeddings\n",
    "\n",
    "Now we'll generate vector embeddings for each chunk using Sentence Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING EMBEDDINGS\n",
      "================================================================================\n",
      "‚úì Mistral client initialized!\n",
      "\n",
      "Generating embeddings for 399 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded\n",
      "Embedding dimension: (399, 1024)\n",
      "\n",
      "‚úì Generated embeddings\n",
      "Embeddings shape: (399, 1024)\n",
      "  ‚Ä¢ 399 chunks\n",
      "  ‚Ä¢ 1024 dimensions per embedding\n",
      "\n",
      "‚úì Embeddings added to chunk metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING EMBEDDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load Mistral API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è  Warning: MISTRAL_API_KEY not found in .env file\")\n",
    "    print(\"Please add your Mistral API key to continue with this section.\")\n",
    "else:\n",
    "    mistral_client = Mistral(api_key=api_key)\n",
    "    print(\"‚úì Mistral client initialized!\")\n",
    "    \n",
    "    \n",
    "# Load embedding model\n",
    "# print(\"\\nLoading Sentence Transformer model...\")\n",
    "# embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "all_embeddings = []\n",
    "# Generate embeddings for all chunks\n",
    "print(f\"\\nGenerating embeddings for {len(all_chunks)} chunks...\")\n",
    "texts = [chunk['text'] for chunk in all_chunks]\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "    \n",
    "    try:\n",
    "        embeddings_response = mistral_client.embeddings.create(\n",
    "            model=\"mistral-embed\",\n",
    "            inputs=batch_texts\n",
    "        )\n",
    "        \n",
    "        batch_embeddings = [item.embedding for item in embeddings_response.data]\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings = np.array(all_embeddings)\n",
    "    \n",
    "embedding_dim = embeddings.shape[1]\n",
    "print(f\"‚úì Model loaded\")\n",
    "print(f\"Embedding dimension: {embeddings.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n‚úì Generated embeddings\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"  ‚Ä¢ {embeddings.shape[0]} chunks\")\n",
    "print(f\"  ‚Ä¢ {embeddings.shape[1]} dimensions per embedding\")\n",
    "\n",
    "# Add embeddings to chunks\n",
    "for chunk, embedding in zip(all_chunks, embeddings):\n",
    "    chunk['embedding'] = embedding\n",
    "\n",
    "print(\"\\n‚úì Embeddings added to chunk metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Store in FAISS Vector Database\n",
    "\n",
    "FAISS is a library for efficient similarity search of high-dimensional vectors.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Index**: The data structure that stores vectors and enables fast search\n",
    "- **IndexFlatL2**: Exact search using L2 (Euclidean) distance\n",
    "- **IndexFlatIP**: Exact search using inner product (cosine similarity)\n",
    "- **IndexIVF**: Approximate search using inverted file indexes (faster, less accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING FAISS INDEX\n",
      "================================================================================\n",
      "\n",
      "Created FAISS IndexFlatL2 with dimension 1024\n",
      "Index is trained: True\n",
      "Number of vectors: 0\n",
      "\n",
      "‚úì Added 399 vectors to FAISS index\n",
      "‚úì Saved FAISS index to 'vector_dbs/faiss_index.bin'\n",
      "‚úì Saved metadata to 'vector_dbs/faiss_metadata.json'\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CREATING FAISS INDEX\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create FAISS index\n",
    "# Using IndexFlatL2 for exact search with L2 distance\n",
    "dimension = embedding_dim\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "print(f\"\\nCreated FAISS IndexFlatL2 with dimension {dimension}\")\n",
    "print(f\"Index is trained: {faiss_index.is_trained}\")\n",
    "print(f\"Number of vectors: {faiss_index.ntotal}\")\n",
    "\n",
    "# Convert embeddings to float32 (FAISS requirement)\n",
    "embeddings_array = np.array([chunk['embedding'] for chunk in all_chunks]).astype('float32')\n",
    "\n",
    "# Add vectors to index\n",
    "faiss_index.add(embeddings_array)\n",
    "\n",
    "print(f\"\\n‚úì Added {faiss_index.ntotal} vectors to FAISS index\")\n",
    "\n",
    "# Save index to disk\n",
    "os.makedirs('vector_dbs', exist_ok=True)\n",
    "faiss.write_index(faiss_index, 'vector_dbs/faiss_index.bin')\n",
    "print(\"‚úì Saved FAISS index to 'vector_dbs/faiss_index.bin'\")\n",
    "\n",
    "# Save metadata separately (FAISS only stores vectors)\n",
    "metadata = []\n",
    "for chunk in all_chunks:\n",
    "    chunk_meta = chunk.copy()\n",
    "    chunk_meta.pop('embedding')  # Remove embedding array for JSON serialization\n",
    "    metadata.append(chunk_meta)\n",
    "\n",
    "with open('vector_dbs/faiss_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úì Saved metadata to 'vector_dbs/faiss_metadata.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Store in LanceDB Vector Database\n",
    "\n",
    "LanceDB is a modern vector database with built-in metadata support and SQL-like queries.\n",
    "\n",
    "### Key Features:\n",
    "- **Persistent**: Data is stored on disk\n",
    "- **Metadata**: Store vectors and metadata together\n",
    "- **SQL-like**: Familiar query syntax\n",
    "- **Versioning**: Track data changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING LANCEDB DATABASE\n",
      "================================================================================\n",
      "\n",
      "‚úì Connected to LanceDB\n",
      "\n",
      "‚úì Created table 'document_chunks' in LanceDB\n",
      "‚úì Added 399 records\n",
      "‚úì Database saved to 'vector_dbs/lancedb'\n",
      "\n",
      "Table schema:\n",
      "  ‚Ä¢ chunk_id: integer\n",
      "  ‚Ä¢ text: string\n",
      "  ‚Ä¢ file_name: string\n",
      "  ‚Ä¢ page_number: integer\n",
      "  ‚Ä¢ chunk_number: integer\n",
      "  ‚Ä¢ char_count: integer\n",
      "  ‚Ä¢ vector: float array[1024]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CREATING LANCEDB DATABASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Connect to LanceDB (creates database if it doesn't exist)\n",
    "lance_db = lancedb.connect('vector_dbs/lancedb')\n",
    "\n",
    "print(\"\\n‚úì Connected to LanceDB\")\n",
    "\n",
    "# Prepare data for LanceDB\n",
    "# LanceDB expects a list of dictionaries with vector and metadata\n",
    "lance_data = []\n",
    "for chunk in all_chunks:\n",
    "    lance_data.append({\n",
    "        'chunk_id': chunk['chunk_id'],\n",
    "        'text': chunk['text'],\n",
    "        'file_name': chunk['file_name'],\n",
    "        'page_number': chunk['page_number'],\n",
    "        'chunk_number': chunk['chunk_number'],\n",
    "        'char_count': chunk['char_count'],\n",
    "        'vector': chunk['embedding'].tolist()  # Convert numpy array to list\n",
    "    })\n",
    "\n",
    "# Create table (or overwrite if exists)\n",
    "table_name = 'document_chunks'\n",
    "try:\n",
    "    # Drop table if it exists\n",
    "    lance_db.drop_table(table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new table\n",
    "table = lance_db.create_table(table_name, data=lance_data)\n",
    "\n",
    "print(f\"\\n‚úì Created table '{table_name}' in LanceDB\")\n",
    "print(f\"‚úì Added {len(lance_data)} records\")\n",
    "print(f\"‚úì Database saved to 'vector_dbs/lancedb'\")\n",
    "\n",
    "# Display table info\n",
    "print(f\"\\nTable schema:\")\n",
    "print(f\"  ‚Ä¢ chunk_id: integer\")\n",
    "print(f\"  ‚Ä¢ text: string\")\n",
    "print(f\"  ‚Ä¢ file_name: string\")\n",
    "print(f\"  ‚Ä¢ page_number: integer\")\n",
    "print(f\"  ‚Ä¢ chunk_number: integer\")\n",
    "print(f\"  ‚Ä¢ char_count: integer\")\n",
    "print(f\"  ‚Ä¢ vector: float array[{embedding_dim}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Retrieval - Semantic Search\n",
    "\n",
    "Now let's implement semantic search using both FAISS and LanceDB.\n",
    "\n",
    "### How Semantic Search Works:\n",
    "1. Convert query text to embedding vector\n",
    "2. Find k-nearest neighbors in vector space\n",
    "3. Return chunks with highest similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Semantic search functions defined\n"
     ]
    }
   ],
   "source": [
    "def semantic_search_faiss(query: str, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform semantic search using FAISS.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with results and scores\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    # query_embedding = embedding_model.encode([query])[0].astype('float32')\n",
    "    # query_embedding = np.array([query_embedding])  # FAISS expects 2D array\n",
    "    \n",
    "# Load Mistral API key\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        print(\"‚ö†Ô∏è  Warning: MISTRAL_API_KEY not found in .env file\")\n",
    "        print(\"Please add your Mistral API key to continue with this section.\")\n",
    "    else:\n",
    "        mistral_client = Mistral(api_key=api_key)\n",
    "        print(\"‚úì Mistral client initialized!\")\n",
    "    \n",
    "    print(f\"Generated {len(embeddings)} embeddings\")\n",
    "    embeddings_response = mistral_client.embeddings.create(\n",
    "            model=\"mistral-embed\",\n",
    "            inputs=[query]\n",
    "        )\n",
    "    \n",
    "    # Extract the actual embedding and convert to numpy array\n",
    "    query_embedding = np.array(embeddings_response.data[0].embedding, dtype='float32')\n",
    "    \n",
    "    # Reshape to 2D array (required by FAISS)\n",
    "    query_embedding = query_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Make it contiguous\n",
    "    query_embedding = np.ascontiguousarray(query_embedding)\n",
    "    \n",
    "    # Normalize for cosine similarity (if your index uses normalized vectors)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "\n",
    "\n",
    "    # Search in FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open('vector_dbs/faiss_metadata.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        result = metadata[idx].copy()\n",
    "        result['distance'] = float(distance)\n",
    "        result['similarity_score'] = 1 / (1 + distance)  # Convert distance to similarity\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def semantic_search_lancedb(query: str, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform semantic search using LanceDB.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with results and scores\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    # query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    # Load Mistral API key\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        print(\"‚ö†Ô∏è  Warning: MISTRAL_API_KEY not found in .env file\")\n",
    "        print(\"Please add your Mistral API key to continue with this section.\")\n",
    "    else:\n",
    "        mistral_client = Mistral(api_key=api_key)\n",
    "        print(\"‚úì Mistral client initialized!\")\n",
    "    \n",
    "    query_embedding = mistral_client.embeddings.create(\n",
    "            model=\"mistral-embed\",\n",
    "            inputs=[query]\n",
    "        )\n",
    "    query_embedding = embeddings_response.data[0].embedding\n",
    "    \n",
    "    \n",
    "    # Open table\n",
    "    table = lance_db.open_table('document_chunks')\n",
    "    \n",
    "    # Search using vector similarity\n",
    "    results = table.search(query_embedding).limit(k).to_list()\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for result in results:\n",
    "        formatted_result = {\n",
    "            'chunk_id': result['chunk_id'],\n",
    "            'text': result['text'],\n",
    "            'file_name': result['file_name'],\n",
    "            'page_number': result['page_number'],\n",
    "            'chunk_number': result['chunk_number'],\n",
    "            'char_count': result['char_count'],\n",
    "            'distance': result['_distance'],\n",
    "            'similarity_score': 1 / (1 + result['_distance'])\n",
    "        }\n",
    "        formatted_results.append(formatted_result)\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "print(\"‚úì Semantic search functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SEMANTIC SEARCH TEST\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: 'agriculture'\n",
      "================================================================================\n",
      "\n",
      "üîç FAISS Results:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Mistral client initialized!\n",
      "Generated 399 embeddings\n",
      "\n",
      "1. [Score: 0.702] Page 7, Chunk 2\n",
      "   t; \n",
      "4) Mining; \n",
      "5) Financial Sector; and \n",
      "6) Regulatory Reforms. \n",
      "Agriculture as the 1st Engine \n",
      "9. Now I move to specific proposals, beginning with ‚ÄòAgriculture as the 1st \n",
      "Engine‚Äô.  \n",
      "Prime Minister ...\n",
      "\n",
      "2. [Score: 0.677] Page 33, Chunk 2\n",
      "   t and businesses for young farmers \n",
      "and rural youth;  \n",
      "3) nurturing and modernizing agriculture for productivity improvement and \n",
      "warehousing, especially for marginal and small farmers; and  \n",
      "4) diver...\n",
      "\n",
      "3. [Score: 0.667] Page 3, Chunk 1\n",
      "   CONTENTS \n",
      " \n",
      "PART ‚Äì A \n",
      " Page No. \n",
      "Introduction 1 \n",
      "Budget Theme 1 \n",
      "Agriculture as the 1st engine 3 \n",
      "MSMEs as the 2nd engine 6 \n",
      "Investment as the 3rd engine 8 \n",
      "A. Investing in People 8 \n",
      "B. Investing in t...\n",
      "\n",
      "\n",
      "üîç LanceDB Results:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Mistral client initialized!\n",
      "\n",
      "1. [Score: 1.000] Page 45, Chunk 4\n",
      "   ation wagons \n",
      "and racing cars, under tariff heading \n",
      "8703 >USD 40000 \n",
      "125  (tariff \n",
      "rate) \n",
      " \n",
      "100 BCD + \n",
      "10 SWS \n",
      "(effective \n",
      "rate) \n",
      "70 (tariff \n",
      "rate) \n",
      " \n",
      "70+ 40 \n",
      "AIDC \n",
      "(effective \n",
      "rate) \n",
      "31. Used Motor ...\n",
      "\n",
      "2. [Score: 0.900] Page 45, Chunk 5\n",
      "   d for the \n",
      "transport of persons, including station \n",
      "wagons and racing cars, under tariff \n",
      "heading 8703 \n",
      "125 (tariff) \n",
      " \n",
      "125 BCD + \n",
      "12.5 SWS \n",
      "(effective \n",
      "rate) \n",
      "70 (tariff) \n",
      " \n",
      "70+ 67.5 \n",
      "AIDC \n",
      "(effectiv...\n",
      "\n",
      "3. [Score: 0.851] Page 46, Chunk 1\n",
      "   42  \n",
      " \n",
      "with or without side -cars under tariff \n",
      "heading 8711 \n",
      "(No change \n",
      "in effective \n",
      "rate) \n",
      "(No change \n",
      "in effective \n",
      "rate) \n",
      "33. Used Motorcycles (including mopeds) \n",
      "and cycles fitted with an auxil...\n",
      "\n",
      "================================================================================\n",
      "Query: 'tax'\n",
      "================================================================================\n",
      "\n",
      "üîç FAISS Results:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Mistral client initialized!\n",
      "Generated 399 embeddings\n",
      "\n",
      "1. [Score: 0.658] Page 50, Chunk 5\n",
      "   ion of tax benefit are given in the table \n",
      "below:...\n",
      "\n",
      "2. [Score: 0.655] Page 28, Chunk 2\n",
      "   s taken steps to understand and address the needs voiced by our \n",
      "citizens. My tax proposals are guided by this spirit.  \n",
      "136. The objectives of my proposals are as follows: \n",
      "(i) Personal Income Tax re...\n",
      "\n",
      "3. [Score: 0.654] Page 27, Chunk 4\n",
      "   ts instead of a monthly statement.  \n",
      " \n",
      "Direct Taxes \n",
      "I now come to my Direct tax proposals.  \n",
      "134.  In Part A, I have briefly underlined Taxation Reforms as one of key \n",
      "reforms to realize our vision o...\n",
      "\n",
      "\n",
      "üîç LanceDB Results:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Mistral client initialized!\n",
      "\n",
      "1. [Score: 1.000] Page 45, Chunk 4\n",
      "   ation wagons \n",
      "and racing cars, under tariff heading \n",
      "8703 >USD 40000 \n",
      "125  (tariff \n",
      "rate) \n",
      " \n",
      "100 BCD + \n",
      "10 SWS \n",
      "(effective \n",
      "rate) \n",
      "70 (tariff \n",
      "rate) \n",
      " \n",
      "70+ 40 \n",
      "AIDC \n",
      "(effective \n",
      "rate) \n",
      "31. Used Motor ...\n",
      "\n",
      "2. [Score: 0.900] Page 45, Chunk 5\n",
      "   d for the \n",
      "transport of persons, including station \n",
      "wagons and racing cars, under tariff \n",
      "heading 8703 \n",
      "125 (tariff) \n",
      " \n",
      "125 BCD + \n",
      "12.5 SWS \n",
      "(effective \n",
      "rate) \n",
      "70 (tariff) \n",
      " \n",
      "70+ 67.5 \n",
      "AIDC \n",
      "(effectiv...\n",
      "\n",
      "3. [Score: 0.851] Page 46, Chunk 1\n",
      "   42  \n",
      " \n",
      "with or without side -cars under tariff \n",
      "heading 8711 \n",
      "(No change \n",
      "in effective \n",
      "rate) \n",
      "(No change \n",
      "in effective \n",
      "rate) \n",
      "33. Used Motorcycles (including mopeds) \n",
      "and cycles fitted with an auxil...\n",
      "\n",
      "================================================================================\n",
      "Query: 'salary'\n",
      "================================================================================\n",
      "\n",
      "üîç FAISS Results:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Mistral client initialized!\n",
      "Generated 399 embeddings\n",
      "\n",
      "1. [Score: 0.655] Page 31, Chunk 6\n",
      "   ayable upto income \n",
      "of ` 12 lakh (i.e. average income of ` 1 lakh per month other than special rate \n",
      "income such as capital gains) under the new regime. This limit will be ` 12.75 \n",
      "lakh for salaried t...\n",
      "\n",
      "2. [Score: 0.634] Page 53, Chunk 1\n",
      "   49  \n",
      " \n",
      "11.  194J - Fee for \n",
      "professional or \n",
      "technical services \n",
      "30,000/- 50,000/- \n",
      "12.  194LA - Income by \n",
      "way of enhanced \n",
      "compensation \n",
      "2,50,000/- 5,00,000/- \n",
      "13.  206C(1G) ‚Äì \n",
      "Remittance under \n",
      "LRS...\n",
      "\n",
      "3. [Score: 0.630] Page 52, Chunk 6\n",
      "   mission or \n",
      "brokerage  \n",
      "15,000/- 20,000/- \n",
      "10.  \n",
      "194-I Rent \n",
      "2,40,000/- during the \n",
      "financial year \n",
      "50,000/- per month or part \n",
      "of a month...\n",
      "\n",
      "\n",
      "üîç LanceDB Results:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Mistral client initialized!\n",
      "\n",
      "1. [Score: 1.000] Page 45, Chunk 4\n",
      "   ation wagons \n",
      "and racing cars, under tariff heading \n",
      "8703 >USD 40000 \n",
      "125  (tariff \n",
      "rate) \n",
      " \n",
      "100 BCD + \n",
      "10 SWS \n",
      "(effective \n",
      "rate) \n",
      "70 (tariff \n",
      "rate) \n",
      " \n",
      "70+ 40 \n",
      "AIDC \n",
      "(effective \n",
      "rate) \n",
      "31. Used Motor ...\n",
      "\n",
      "2. [Score: 0.900] Page 45, Chunk 5\n",
      "   d for the \n",
      "transport of persons, including station \n",
      "wagons and racing cars, under tariff \n",
      "heading 8703 \n",
      "125 (tariff) \n",
      " \n",
      "125 BCD + \n",
      "12.5 SWS \n",
      "(effective \n",
      "rate) \n",
      "70 (tariff) \n",
      " \n",
      "70+ 67.5 \n",
      "AIDC \n",
      "(effectiv...\n",
      "\n",
      "3. [Score: 0.851] Page 46, Chunk 1\n",
      "   42  \n",
      " \n",
      "with or without side -cars under tariff \n",
      "heading 8711 \n",
      "(No change \n",
      "in effective \n",
      "rate) \n",
      "(No change \n",
      "in effective \n",
      "rate) \n",
      "33. Used Motorcycles (including mopeds) \n",
      "and cycles fitted with an auxil...\n"
     ]
    }
   ],
   "source": [
    "# Test semantic search\n",
    "print(\"=\" * 80)\n",
    "print(\"SEMANTIC SEARCH TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_queries = [\n",
    "    \"agriculture\",\n",
    "    \"tax\",\n",
    "    \"salary\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Search with FAISS\n",
    "    print(\"\\nüîç FAISS Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    faiss_results = semantic_search_faiss(query, k=3)\n",
    "    \n",
    "    for i, result in enumerate(faiss_results, 1):\n",
    "        print(f\"\\n{i}. [Score: {result['similarity_score']:.3f}] \"\n",
    "              f\"Page {result['page_number']}, Chunk {result['chunk_number']}\")\n",
    "        print(f\"   {result['text'][:200]}...\")\n",
    "    \n",
    "    # Search with LanceDB\n",
    "    print(\"\\n\\nüîç LanceDB Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    lance_results = semantic_search_lancedb(query, k=3)\n",
    "    \n",
    "    for i, result in enumerate(lance_results, 1):\n",
    "        print(f\"\\n{i}. [Score: {result['similarity_score']:.3f}] \"\n",
    "              f\"Page {result['page_number']}, Chunk {result['chunk_number']}\")\n",
    "        print(f\"   {result['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Retrieval - Keyword Search\n",
    "\n",
    "Traditional keyword search looks for exact text matches.\n",
    "\n",
    "### Keyword Search vs Semantic Search:\n",
    "\n",
    "| Aspect | Keyword Search | Semantic Search |\n",
    "|--------|----------------|------------------|\n",
    "| Matching | Exact text match | Meaning-based |\n",
    "| Synonyms | Misses synonyms | Finds synonyms |\n",
    "| Context | No understanding | Context-aware |\n",
    "| Speed | Very fast | Fast (with index) |\n",
    "| Use Case | Known terms | Natural questions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Keyword search functions defined\n"
     ]
    }
   ],
   "source": [
    "def keyword_search(query: str, chunks: List[Dict], top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform simple keyword search.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        chunks: List of chunks to search\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of matching chunks with scores\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    query_terms = query_lower.split()\n",
    "    \n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        text_lower = chunk['text'].lower()\n",
    "        \n",
    "        # Count term matches\n",
    "        matches = sum(1 for term in query_terms if term in text_lower)\n",
    "        \n",
    "        if matches > 0:\n",
    "            score = matches / len(query_terms)  # Normalized score\n",
    "            result = chunk.copy()\n",
    "            result.pop('embedding', None)\n",
    "            result['keyword_score'] = score\n",
    "            result['matched_terms'] = matches\n",
    "            results.append(result)\n",
    "    \n",
    "    # Sort by score\n",
    "    results.sort(key=lambda x: x['keyword_score'], reverse=True)\n",
    "    \n",
    "    return results[:top_k]\n",
    "\n",
    "\n",
    "def keyword_search_lancedb(query: str, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform keyword search using LanceDB SQL-like queries.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of matching chunks\n",
    "    \"\"\"\n",
    "    table = lance_db.open_table('document_chunks')\n",
    "    \n",
    "    # Use SQL-like WHERE clause for text search\n",
    "    # Note: This is a simple contains search\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # LanceDB doesn't have full-text search built-in, so we'll get all and filter\n",
    "    all_results = table.to_pandas()\n",
    "    \n",
    "    # Filter based on keyword presence\n",
    "    query_terms = query_lower.split()\n",
    "    matches = []\n",
    "    \n",
    "    for _, row in all_results.iterrows():\n",
    "        text_lower = row['text'].lower()\n",
    "        matched_terms = sum(1 for term in query_terms if term in text_lower)\n",
    "        \n",
    "        if matched_terms > 0:\n",
    "            result = row.to_dict()\n",
    "            result.pop('vector', None)  # Remove vector for display\n",
    "            result['keyword_score'] = matched_terms / len(query_terms)\n",
    "            result['matched_terms'] = matched_terms\n",
    "            matches.append(result)\n",
    "    \n",
    "    # Sort by score\n",
    "    matches.sort(key=lambda x: x['keyword_score'], reverse=True)\n",
    "    \n",
    "    return matches[:k]\n",
    "\n",
    "print(\"‚úì Keyword search functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KEYWORD SEARCH TEST\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: 'agriculture'\n",
      "================================================================================\n",
      "\n",
      "üîé Keyword Search Results (In-Memory):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 1.00, Matched: 1 terms] Page 3, Chunk 1\n",
      "   CONTENTS \n",
      " \n",
      "PART ‚Äì A \n",
      " Page No. \n",
      "Introduction 1 \n",
      "Budget Theme 1 \n",
      "Agriculture as the 1st engine 3 \n",
      "MSMEs as the 2nd engine 6 \n",
      "Investment as the 3rd engine 8 \n",
      "A. Investing in People 8 \n",
      "B. Investing in t...\n",
      "\n",
      "2. [Score: 1.00, Matched: 1 terms] Page 6, Chunk 6\n",
      "   7. For this journey of development,  \n",
      "a) Our four powerful engines are: Agriculture, MSME, Investment, and \n",
      "Exports  \n",
      "b) The fuel: our Reforms  \n",
      "c) Our guiding spirit: Inclusivity \n",
      "d) And the destinat...\n",
      "\n",
      "3. [Score: 1.00, Matched: 1 terms] Page 7, Chunk 2\n",
      "   t; \n",
      "4) Mining; \n",
      "5) Financial Sector; and \n",
      "6) Regulatory Reforms. \n",
      "Agriculture as the 1st Engine \n",
      "9. Now I move to specific proposals, beginning with ‚ÄòAgriculture as the 1st \n",
      "Engine‚Äô.  \n",
      "Prime Minister ...\n",
      "\n",
      "\n",
      "üîé Keyword Search Results (LanceDB):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 1.00, Matched: 1 terms] Page 3, Chunk 1\n",
      "   CONTENTS \n",
      " \n",
      "PART ‚Äì A \n",
      " Page No. \n",
      "Introduction 1 \n",
      "Budget Theme 1 \n",
      "Agriculture as the 1st engine 3 \n",
      "MSMEs as the 2nd engine 6 \n",
      "Investment as the 3rd engine 8 \n",
      "A. Investing in People 8 \n",
      "B. Investing in t...\n",
      "\n",
      "2. [Score: 1.00, Matched: 1 terms] Page 6, Chunk 6\n",
      "   7. For this journey of development,  \n",
      "a) Our four powerful engines are: Agriculture, MSME, Investment, and \n",
      "Exports  \n",
      "b) The fuel: our Reforms  \n",
      "c) Our guiding spirit: Inclusivity \n",
      "d) And the destinat...\n",
      "\n",
      "3. [Score: 1.00, Matched: 1 terms] Page 7, Chunk 2\n",
      "   t; \n",
      "4) Mining; \n",
      "5) Financial Sector; and \n",
      "6) Regulatory Reforms. \n",
      "Agriculture as the 1st Engine \n",
      "9. Now I move to specific proposals, beginning with ‚ÄòAgriculture as the 1st \n",
      "Engine‚Äô.  \n",
      "Prime Minister ...\n",
      "\n",
      "================================================================================\n",
      "Query: 'tax'\n",
      "================================================================================\n",
      "\n",
      "üîé Keyword Search Results (In-Memory):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 1.00, Matched: 1 terms] Page 3, Chunk 2\n",
      "   ports as the 4th engine 15 \n",
      "Reforms as the Fuel 16 \n",
      "Fiscal Policy 18 \n",
      " \n",
      " \n",
      "PART ‚Äì B \n",
      "Indirect taxes 20 \n",
      "Direct Taxes  23 \n",
      " \n",
      "Annexure to Part-A 29 \n",
      "Annexure to Part-B 31...\n",
      "\n",
      "2. [Score: 1.00, Matched: 1 terms] Page 7, Chunk 1\n",
      "   3  \n",
      " \n",
      "8. This Budget aims to initiate transformative reforms across six domains. \n",
      "During the next five years, these will augment our growth potential and global \n",
      "competitiveness. The domains are:  \n",
      "1)...\n",
      "\n",
      "3. [Score: 1.00, Matched: 1 terms] Page 20, Chunk 2\n",
      "   e and \n",
      "warehousing for air cargo including high value perishable horticulture produce. \n",
      "Cargo screening and customs protocols will be streamlined and made user -\n",
      "friendly.      \n",
      "Reforms as the Fuel \n",
      "9...\n",
      "\n",
      "\n",
      "üîé Keyword Search Results (LanceDB):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 1.00, Matched: 1 terms] Page 3, Chunk 2\n",
      "   ports as the 4th engine 15 \n",
      "Reforms as the Fuel 16 \n",
      "Fiscal Policy 18 \n",
      " \n",
      " \n",
      "PART ‚Äì B \n",
      "Indirect taxes 20 \n",
      "Direct Taxes  23 \n",
      " \n",
      "Annexure to Part-A 29 \n",
      "Annexure to Part-B 31...\n",
      "\n",
      "2. [Score: 1.00, Matched: 1 terms] Page 7, Chunk 1\n",
      "   3  \n",
      " \n",
      "8. This Budget aims to initiate transformative reforms across six domains. \n",
      "During the next five years, these will augment our growth potential and global \n",
      "competitiveness. The domains are:  \n",
      "1)...\n",
      "\n",
      "3. [Score: 1.00, Matched: 1 terms] Page 20, Chunk 2\n",
      "   e and \n",
      "warehousing for air cargo including high value perishable horticulture produce. \n",
      "Cargo screening and customs protocols will be streamlined and made user -\n",
      "friendly.      \n",
      "Reforms as the Fuel \n",
      "9...\n",
      "\n",
      "================================================================================\n",
      "Query: 'salary'\n",
      "================================================================================\n",
      "\n",
      "üîé Keyword Search Results (In-Memory):\n",
      "--------------------------------------------------------------------------------\n",
      "No matches found.\n",
      "\n",
      "\n",
      "üîé Keyword Search Results (LanceDB):\n",
      "--------------------------------------------------------------------------------\n",
      "No matches found.\n"
     ]
    }
   ],
   "source": [
    "# Test keyword search\n",
    "print(\"=\" * 80)\n",
    "print(\"KEYWORD SEARCH TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_keyword_queries = [\n",
    "    \"agriculture\",\n",
    "    \"tax\",\n",
    "    \"salary\"\n",
    "]\n",
    "\n",
    "\n",
    "for query in test_keyword_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Keyword search (simple)\n",
    "    print(\"\\nüîé Keyword Search Results (In-Memory):\")\n",
    "    print(\"-\" * 80)\n",
    "    keyword_results = keyword_search(query, all_chunks, top_k=3)\n",
    "    \n",
    "    if keyword_results:\n",
    "        for i, result in enumerate(keyword_results, 1):\n",
    "            print(f\"\\n{i}. [Score: {result['keyword_score']:.2f}, \"\n",
    "                  f\"Matched: {result['matched_terms']} terms] \"\n",
    "                  f\"Page {result['page_number']}, Chunk {result['chunk_number']}\")\n",
    "            print(f\"   {result['text'][:200]}...\")\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "    \n",
    "    # Keyword search with LanceDB\n",
    "    print(\"\\n\\nüîé Keyword Search Results (LanceDB):\")\n",
    "    print(\"-\" * 80)\n",
    "    lance_keyword_results = keyword_search_lancedb(query, k=3)\n",
    "    \n",
    "    if lance_keyword_results:\n",
    "        for i, result in enumerate(lance_keyword_results, 1):\n",
    "            print(f\"\\n{i}. [Score: {result['keyword_score']:.2f}, \"\n",
    "                  f\"Matched: {result['matched_terms']} terms] \"\n",
    "                  f\"Page {result['page_number']}, Chunk {result['chunk_number']}\")\n",
    "            print(f\"   {result['text'][:200]}...\")\n",
    "    else:\n",
    "        print(\"No matches found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Comparison - Keyword vs Semantic Search\n",
    "\n",
    "Let's compare how the two search methods perform on the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KEYWORD VS SEMANTIC SEARCH COMPARISON\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: 'agriculture'\n",
      "Type: Query with no exact keyword matches\n",
      "================================================================================\n",
      "\n",
      "üîé KEYWORD SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Score: 1.00\n",
      "   CONTENTS \n",
      " \n",
      "PART ‚Äì A \n",
      " Page No. \n",
      "Introduction 1 \n",
      "Budget Theme 1 \n",
      "Agriculture as the 1st engine 3 \n",
      "MSMEs as the 2nd engine 6 \n",
      "Investment as the 3rd eng...\n",
      "\n",
      "2. Score: 1.00\n",
      "   7. For this journey of development,  \n",
      "a) Our four powerful engines are: Agriculture, MSME, Investment, and \n",
      "Exports  \n",
      "b) The fuel: our Reforms  \n",
      "c) Ou...\n",
      "\n",
      "\n",
      "üîç SEMANTIC SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Mistral client initialized!\n",
      "Generated 399 embeddings\n",
      "\n",
      "1. Score: 0.702\n",
      "   t; \n",
      "4) Mining; \n",
      "5) Financial Sector; and \n",
      "6) Regulatory Reforms. \n",
      "Agriculture as the 1st Engine \n",
      "9. Now I move to specific proposals, beginning with ‚Äò...\n",
      "\n",
      "2. Score: 0.677\n",
      "   t and businesses for young farmers \n",
      "and rural youth;  \n",
      "3) nurturing and modernizing agriculture for productivity improvement and \n",
      "warehousing, especia...\n",
      "\n",
      "================================================================================\n",
      "üìä Analysis:\n",
      "  ‚Ä¢ Both methods found results\n",
      "  ‚Ä¢ Semantic search may find more relevant context\n",
      "\n",
      "================================================================================\n",
      "Query: 'tax'\n",
      "Type: Query with exact keywords\n",
      "================================================================================\n",
      "\n",
      "üîé KEYWORD SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Score: 1.00\n",
      "   ports as the 4th engine 15 \n",
      "Reforms as the Fuel 16 \n",
      "Fiscal Policy 18 \n",
      " \n",
      " \n",
      "PART ‚Äì B \n",
      "Indirect taxes 20 \n",
      "Direct Taxes  23 \n",
      " \n",
      "Annexure to Part-A 29 \n",
      "Anne...\n",
      "\n",
      "2. Score: 1.00\n",
      "   3  \n",
      " \n",
      "8. This Budget aims to initiate transformative reforms across six domains. \n",
      "During the next five years, these will augment our growth potential ...\n",
      "\n",
      "\n",
      "üîç SEMANTIC SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Mistral client initialized!\n",
      "Generated 399 embeddings\n",
      "\n",
      "1. Score: 0.658\n",
      "   ion of tax benefit are given in the table \n",
      "below:...\n",
      "\n",
      "2. Score: 0.655\n",
      "   s taken steps to understand and address the needs voiced by our \n",
      "citizens. My tax proposals are guided by this spirit.  \n",
      "136. The objectives of my pro...\n",
      "\n",
      "================================================================================\n",
      "üìä Analysis:\n",
      "  ‚Ä¢ Both methods found results\n",
      "  ‚Ä¢ Semantic search may find more relevant context\n",
      "\n",
      "================================================================================\n",
      "Query: 'salary'\n",
      "Type: Natural language question\n",
      "================================================================================\n",
      "\n",
      "üîé KEYWORD SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå No matches found\n",
      "\n",
      "\n",
      "üîç SEMANTIC SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Mistral client initialized!\n",
      "Generated 399 embeddings\n",
      "\n",
      "1. Score: 0.655\n",
      "   ayable upto income \n",
      "of ` 12 lakh (i.e. average income of ` 1 lakh per month other than special rate \n",
      "income such as capital gains) under the new regim...\n",
      "\n",
      "2. Score: 0.634\n",
      "   49  \n",
      " \n",
      "11.  194J - Fee for \n",
      "professional or \n",
      "technical services \n",
      "30,000/- 50,000/- \n",
      "12.  194LA - Income by \n",
      "way of enhanced \n",
      "compensation \n",
      "2,50,000/- ...\n",
      "\n",
      "================================================================================\n",
      "üìä Analysis:\n",
      "  ‚Ä¢ Keyword search failed (no exact matches)\n",
      "  ‚Ä¢ Semantic search succeeded (understood meaning)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KEYWORD VS SEMANTIC SEARCH COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "comparison_queries = [\n",
    "    (\"agriculture\", \"Query with no exact keyword matches\"),\n",
    "    (\"tax\", \"Query with exact keywords\"),\n",
    "    (\"salary\", \"Natural language question\")\n",
    "]\n",
    "\n",
    "for query, description in comparison_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Type: {description}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Keyword search\n",
    "    print(\"\\nüîé KEYWORD SEARCH:\")\n",
    "    print(\"-\" * 80)\n",
    "    keyword_results = keyword_search(query, all_chunks, top_k=2)\n",
    "    \n",
    "    if keyword_results:\n",
    "        for i, result in enumerate(keyword_results, 1):\n",
    "            print(f\"\\n{i}. Score: {result['keyword_score']:.2f}\")\n",
    "            print(f\"   {result['text'][:150]}...\")\n",
    "    else:\n",
    "        print(\"‚ùå No matches found\")\n",
    "    \n",
    "    # Semantic search\n",
    "    print(\"\\n\\nüîç SEMANTIC SEARCH:\")\n",
    "    print(\"-\" * 80)\n",
    "    semantic_results = semantic_search_faiss(query, k=2)\n",
    "    \n",
    "    for i, result in enumerate(semantic_results, 1):\n",
    "        print(f\"\\n{i}. Score: {result['similarity_score']:.3f}\")\n",
    "        print(f\"   {result['text'][:150]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Analysis:\")\n",
    "    if not keyword_results:\n",
    "        print(\"  ‚Ä¢ Keyword search failed (no exact matches)\")\n",
    "        print(\"  ‚Ä¢ Semantic search succeeded (understood meaning)\")\n",
    "    elif len(keyword_results) < len(semantic_results):\n",
    "        print(\"  ‚Ä¢ Keyword search found fewer results\")\n",
    "        print(\"  ‚Ä¢ Semantic search more comprehensive\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Both methods found results\")\n",
    "        print(\"  ‚Ä¢ Semantic search may find more relevant context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Hybrid Search (Best of Both Worlds)\n",
    "\n",
    "Combine keyword and semantic search for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Hybrid search function defined\n"
     ]
    }
   ],
   "source": [
    "def hybrid_search(query: str, k: int = 5, keyword_weight: float = 0.3, \n",
    "                 semantic_weight: float = 0.7) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Combine keyword and semantic search with weighted scoring.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        k: Number of results\n",
    "        keyword_weight: Weight for keyword scores\n",
    "        semantic_weight: Weight for semantic scores\n",
    "    \n",
    "    Returns:\n",
    "        List of results with hybrid scores\n",
    "    \"\"\"\n",
    "    # Get both types of results\n",
    "    keyword_results = keyword_search(query, all_chunks, top_k=k*2)\n",
    "    semantic_results = semantic_search_faiss(query, k=k*2)\n",
    "    \n",
    "    # Create score dictionary\n",
    "    hybrid_scores = {}\n",
    "    \n",
    "    # Add keyword scores\n",
    "    for result in keyword_results:\n",
    "        chunk_id = result['chunk_id']\n",
    "        hybrid_scores[chunk_id] = {\n",
    "            'chunk': result,\n",
    "            'keyword_score': result['keyword_score'],\n",
    "            'semantic_score': 0.0\n",
    "        }\n",
    "    \n",
    "    # Add/update with semantic scores\n",
    "    for result in semantic_results:\n",
    "        chunk_id = result['chunk_id']\n",
    "        if chunk_id in hybrid_scores:\n",
    "            hybrid_scores[chunk_id]['semantic_score'] = result['similarity_score']\n",
    "        else:\n",
    "            hybrid_scores[chunk_id] = {\n",
    "                'chunk': result,\n",
    "                'keyword_score': 0.0,\n",
    "                'semantic_score': result['similarity_score']\n",
    "            }\n",
    "    \n",
    "    # Calculate hybrid scores\n",
    "    results = []\n",
    "    for chunk_id, scores in hybrid_scores.items():\n",
    "        hybrid_score = (keyword_weight * scores['keyword_score'] + \n",
    "                       semantic_weight * scores['semantic_score'])\n",
    "        \n",
    "        result = scores['chunk'].copy()\n",
    "        result['keyword_score'] = scores['keyword_score']\n",
    "        result['semantic_score'] = scores['semantic_score']\n",
    "        result['hybrid_score'] = hybrid_score\n",
    "        results.append(result)\n",
    "    \n",
    "    # Sort by hybrid score\n",
    "    results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "    \n",
    "    return results[:k]\n",
    "\n",
    "print(\"‚úì Hybrid search function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HYBRID SEARCH TEST\n",
      "================================================================================\n",
      "\n",
      "Query: 'agriculture'\n",
      "================================================================================\n",
      "‚úì Mistral client initialized!\n",
      "Generated 399 embeddings\n",
      "\n",
      "üîÄ HYBRID SEARCH RESULTS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Hybrid Score: 0.791\n",
      "   ‚îî‚îÄ Keyword: 1.000 | Semantic: 0.702\n",
      "   Page 7, Chunk 2\n",
      "   t; \n",
      "4) Mining; \n",
      "5) Financial Sector; and \n",
      "6) Regulatory Reforms. \n",
      "Agriculture as the 1st Engine \n",
      "9. Now I move to specific proposals, beginning with ‚ÄòAgriculture as the 1st \n",
      "Engine‚Äô.  \n",
      "Prime Minister ...\n",
      "\n",
      "2. Hybrid Score: 0.774\n",
      "   ‚îî‚îÄ Keyword: 1.000 | Semantic: 0.677\n",
      "   Page 33, Chunk 2\n",
      "   t and businesses for young farmers \n",
      "and rural youth;  \n",
      "3) nurturing and modernizing agriculture for productivity improvement and \n",
      "warehousing, especially for marginal and small farmers; and  \n",
      "4) diver...\n",
      "\n",
      "3. Hybrid Score: 0.767\n",
      "   ‚îî‚îÄ Keyword: 1.000 | Semantic: 0.667\n",
      "   Page 3, Chunk 1\n",
      "   CONTENTS \n",
      " \n",
      "PART ‚Äì A \n",
      " Page No. \n",
      "Introduction 1 \n",
      "Budget Theme 1 \n",
      "Agriculture as the 1st engine 3 \n",
      "MSMEs as the 2nd engine 6 \n",
      "Investment as the 3rd engine 8 \n",
      "A. Investing in People 8 \n",
      "B. Investing in t...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Hybrid search combines:\n",
      "  ‚Ä¢ Keyword matching for exact term relevance\n",
      "  ‚Ä¢ Semantic understanding for context and meaning\n",
      "  ‚Ä¢ Weighted scoring for balanced results\n"
     ]
    }
   ],
   "source": [
    "# Test hybrid search\n",
    "print(\"=\" * 80)\n",
    "print(\"HYBRID SEARCH TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query = \"agriculture\"\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hybrid_results = hybrid_search(query, k=3)\n",
    "\n",
    "print(\"\\nüîÄ HYBRID SEARCH RESULTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"\\n{i}. Hybrid Score: {result['hybrid_score']:.3f}\")\n",
    "    print(f\"   ‚îî‚îÄ Keyword: {result['keyword_score']:.3f} | \"\n",
    "          f\"Semantic: {result['semantic_score']:.3f}\")\n",
    "    print(f\"   Page {result['page_number']}, Chunk {result['chunk_number']}\")\n",
    "    print(f\"   {result['text'][:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Hybrid search combines:\")\n",
    "print(\"  ‚Ä¢ Keyword matching for exact term relevance\")\n",
    "print(\"  ‚Ä¢ Semantic understanding for context and meaning\")\n",
    "print(\"  ‚Ä¢ Weighted scoring for balanced results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Loading Saved Indexes\n",
    "\n",
    "Demonstrate how to load previously saved vector databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING SAVED VECTOR DATABASES\n",
      "================================================================================\n",
      "\n",
      "üìÇ Loading FAISS index...\n",
      "‚úì Loaded FAISS index with 399 vectors\n",
      "‚úì Loaded 399 metadata records\n",
      "\n",
      "üìÇ Loading LanceDB...\n",
      "‚úì Loaded LanceDB with 399 records\n",
      "\n",
      "üîç Testing search with loaded indexes...\n",
      "Query: 'machine learning'\n",
      "\n",
      "FAISS Results:\n",
      "  ‚Ä¢ Score: 0.568 - Artificial Intelligence for education will be set up with a total outlay of ` 500 \n",
      "crore.      \n",
      "Expa...\n",
      "  ‚Ä¢ Score: 0.622 - 5....\n",
      "\n",
      "LanceDB Results:\n",
      "  ‚Ä¢ Distance: 0.568 - Artificial Intelligence for education will be set up with a total outlay of ` 500 \n",
      "crore.      \n",
      "Expa...\n",
      "  ‚Ä¢ Distance: 0.622 - 5....\n",
      "\n",
      "‚úÖ Successfully loaded and searched both vector databases!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOADING SAVED VECTOR DATABASES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load FAISS index\n",
    "print(\"\\nüìÇ Loading FAISS index...\")\n",
    "loaded_faiss_index = faiss.read_index('vector_dbs/faiss_index.bin')\n",
    "print(f\"‚úì Loaded FAISS index with {loaded_faiss_index.ntotal} vectors\")\n",
    "\n",
    "# Load FAISS metadata\n",
    "with open('vector_dbs/faiss_metadata.json', 'r') as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "print(f\"‚úì Loaded {len(loaded_metadata)} metadata records\")\n",
    "\n",
    "# Connect to LanceDB\n",
    "print(\"\\nüìÇ Loading LanceDB...\")\n",
    "loaded_lance_db = lancedb.connect('vector_dbs/lancedb')\n",
    "loaded_table = loaded_lance_db.open_table('document_chunks')\n",
    "record_count = loaded_table.count_rows()\n",
    "print(f\"‚úì Loaded LanceDB with {record_count} records\")\n",
    "\n",
    "# Test search with loaded indexes\n",
    "print(\"\\nüîç Testing search with loaded indexes...\")\n",
    "test_query = \"machine learning\"\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "\n",
    "# Get query embedding from Mistral\n",
    "embeddings_response = mistral_client.embeddings.create(\n",
    "    model=\"mistral-embed\",\n",
    "    inputs=[test_query]\n",
    ")\n",
    "\n",
    "# Extract embedding\n",
    "query_embedding_raw = embeddings_response.data[0].embedding\n",
    "\n",
    "# Search loaded FAISS\n",
    "# Convert to numpy array, reshape, and ensure float32\n",
    "query_embedding_faiss = np.array(query_embedding_raw, dtype='float32').reshape(1, -1)\n",
    "query_embedding_faiss = np.ascontiguousarray(query_embedding_faiss)\n",
    "faiss.normalize_L2(query_embedding_faiss)\n",
    "\n",
    "distances, indices = loaded_faiss_index.search(query_embedding_faiss, 2)\n",
    "\n",
    "print(\"FAISS Results:\")\n",
    "for idx, distance in zip(indices[0], distances[0]):\n",
    "    if idx < len(loaded_metadata):\n",
    "        print(f\"  ‚Ä¢ Score: {distance:.3f} - {loaded_metadata[idx]['text'][:100]}...\")\n",
    "\n",
    "# Search loaded LanceDB\n",
    "# Use the raw embedding list directly\n",
    "lance_results = loaded_table.search(query_embedding_raw).limit(2).to_list()\n",
    "\n",
    "print(\"\\nLanceDB Results:\")\n",
    "for result in lance_results:\n",
    "    print(f\"  ‚Ä¢ Distance: {result['_distance']:.3f} - {result['text'][:100]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Successfully loaded and searched both vector databases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### üéØ What We Learned\n",
    "\n",
    "1. **Vector Databases** enable efficient similarity search at scale\n",
    "2. **FAISS** provides fast in-memory vector search\n",
    "3. **LanceDB** offers persistent storage with metadata support\n",
    "\n",
    "### üìä Comparison: FAISS vs LanceDB\n",
    "\n",
    "| Feature | FAISS | LanceDB |\n",
    "|---------|-------|----------|\n",
    "| **Storage** | In-memory (with save/load) | Disk-based, persistent |\n",
    "| **Metadata** | Separate storage required | Built-in support |\n",
    "| **Speed** | Extremely fast | Fast |\n",
    "| **Scalability** | Limited by RAM | Scales to disk |\n",
    "| **Queries** | Vector search only | Vector + SQL-like |\n",
    "| **Use Case** | Research, prototypes | Production apps |\n",
    "| **Updates** | Rebuild index | Easy updates |\n",
    "| **Versioning** | Manual | Built-in |\n",
    "\n",
    "### üîç Search Methods Comparison\n",
    "\n",
    "| Method | Pros | Cons | Best For |\n",
    "|--------|------|------|----------|\n",
    "| **Keyword** | Fast, exact matches | Misses synonyms, no context | Known terms, filters |\n",
    "| **Semantic** | Understands meaning | Slower, needs embeddings | Natural questions |\n",
    "| **Hybrid** | Best of both worlds | More complex | Production systems |\n",
    "\n",
    "### üöÄ Production Considerations\n",
    "\n",
    "1. **Chunking Strategy**: Balance context vs specificity (300-500 chars)\n",
    "2. **Overlap**: Use 10-20% overlap to maintain context\n",
    "3. **Metadata**: Store page, chunk, file info for citations\n",
    "4. **Index Type**: Choose based on scale and accuracy needs\n",
    "5. **Hybrid Search**: Combine methods for best results\n",
    "\n",
    "### üõ†Ô∏è Next Steps\n",
    "\n",
    "- Implement RAG pipeline with vector database\n",
    "- Add filtering by metadata\n",
    "- Experiment with different chunking strategies\n",
    "- Try other vector databases (Chroma, Pinecone, Weaviate)\n",
    "- Implement re-ranking for better results\n",
    "\n",
    "### üìö Additional Vector Databases\n",
    "\n",
    "- **Chroma**: Simple, embedded, great for prototypes\n",
    "- **Pinecone**: Managed, cloud-native, production-ready\n",
    "- **Weaviate**: GraphQL API, hybrid search built-in\n",
    "- **Qdrant**: Rust-based, high performance\n",
    "- **Milvus**: Large-scale, distributed\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Conclusion\n",
    "\n",
    "Vector databases are the foundation of modern AI applications. They enable:\n",
    "- **Fast semantic search** across millions of documents\n",
    "- **Efficient RAG** pipelines for LLMs\n",
    "- **Scalable** similarity search\n",
    "- **Flexible** metadata filtering\n",
    "\n",
    "Choose the right vector database for your use case:\n",
    "- **Prototype/Research**: FAISS, Chroma\n",
    "- **Production**: LanceDB, Pinecone, Weaviate\n",
    "- **Large Scale**: Milvus, Qdrant\n",
    "\n",
    "**The combination of embeddings + vector databases powers the next generation of AI applications!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
