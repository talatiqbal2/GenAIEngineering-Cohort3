{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Vector Databases\n",
    "\n",
    "This notebook demonstrates how to use vector databases for efficient semantic search and retrieval.\n",
    "\n",
    "## What is a Vector Database?\n",
    "\n",
    "A **vector database** is a specialized database designed to store, index, and search high-dimensional vectors (embeddings) efficiently.\n",
    "\n",
    "### Why Do We Need Vector Databases?\n",
    "\n",
    "Traditional databases (SQL, NoSQL) are great for exact matches:\n",
    "- `WHERE name = 'John'`\n",
    "- `WHERE price > 100`\n",
    "\n",
    "But they struggle with:\n",
    "- Finding similar meanings: \"laptop\" vs \"notebook computer\"\n",
    "- Semantic search: \"affordable portable computers\" â†’ find laptops\n",
    "- Nearest neighbor search in high-dimensional space\n",
    "\n",
    "### Vector Database Capabilities:\n",
    "\n",
    "1. **Efficient Storage**: Store millions of high-dimensional vectors\n",
    "2. **Fast Similarity Search**: Find nearest neighbors in milliseconds\n",
    "3. **Metadata Filtering**: Combine semantic search with traditional filters\n",
    "4. **Scalability**: Handle large-scale applications\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **RAG (Retrieval Augmented Generation)**: Find relevant context for LLMs\n",
    "- **Semantic Search**: Search by meaning, not keywords\n",
    "- **Recommendation Systems**: Find similar items\n",
    "- **Duplicate Detection**: Find similar documents\n",
    "- **Question Answering**: Match questions to answers\n",
    "\n",
    "---\n",
    "\n",
    "## Vector Databases We'll Explore\n",
    "\n",
    "### 1. FAISS (Facebook AI Similarity Search)\n",
    "- **Type**: In-memory vector search library\n",
    "- **Best For**: Fast similarity search, research, prototyping\n",
    "- **Pros**: Extremely fast, battle-tested, many index types\n",
    "- **Cons**: In-memory only, no native persistence features\n",
    "\n",
    "### 2. LanceDB\n",
    "- **Type**: Embedded vector database\n",
    "- **Best For**: Production applications, persistent storage\n",
    "- **Pros**: Disk-based, SQL-like queries, versioning, cloud-native\n",
    "- **Cons**: Newer, smaller community\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "! pip install faiss-cpu lancedb sentence-transformers pypdf python-dotenv mistralai pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PDF processing\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Vector databases\n",
    "import faiss\n",
    "import lancedb\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: PDF Parsing and Chunking\n",
    "\n",
    "First, we need to extract text from PDFs and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PDF parsing and chunking functions defined\n"
     ]
    }
   ],
   "source": [
    "def parse_pdf(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse PDF and extract text with metadata.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with page text and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ“„ Parsing PDF: {pdf_path}\")\n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages_data = []\n",
    "    \n",
    "    for page_num, page in enumerate(reader.pages, start=1):\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        if text.strip():  # Only add if there's actual text\n",
    "            pages_data.append({\n",
    "                'page_number': page_num,\n",
    "                'text': text,\n",
    "                'file_name': Path(pdf_path).name\n",
    "            })\n",
    "    \n",
    "    print(f\"âœ“ Extracted text from {len(pages_data)} pages\")\n",
    "    return pages_data\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks with sentence boundary awareness.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_len = len(text)\n",
    "    \n",
    "    while start < text_len:\n",
    "        # Define the end of this chunk\n",
    "        end = min(start + chunk_size, text_len)\n",
    "        \n",
    "        # Extract chunk\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Try to break at sentence boundary (only if not at the very end)\n",
    "        if end < text_len:\n",
    "            # Look for sentence boundaries\n",
    "            last_period = chunk.rfind('.')\n",
    "            last_newline = chunk.rfind('\\n')\n",
    "            last_question = chunk.rfind('?')\n",
    "            last_exclamation = chunk.rfind('!')\n",
    "            \n",
    "            break_point = max(last_period, last_newline, last_question, last_exclamation)\n",
    "            \n",
    "            # Only use the break point if it's reasonably far into the chunk\n",
    "            if break_point > chunk_size * 0.5:\n",
    "                chunk = chunk[:break_point + 1]\n",
    "        \n",
    "        # Add chunk if it has content\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk.strip())\n",
    "        \n",
    "        # Move start position forward by (chunk_size - overlap)\n",
    "        # This guarantees we make progress even with short adjusted chunks\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_pdf_to_chunks(pdf_path: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse PDF and split into chunks with metadata.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with chunks and metadata\n",
    "    \"\"\"\n",
    "    pages_data = parse_pdf(pdf_path)\n",
    "    all_chunks = []\n",
    "    chunk_id = 0\n",
    "    \n",
    "    for page_data in pages_data:\n",
    "        chunks = chunk_text(page_data['text'], chunk_size, overlap)\n",
    "        \n",
    "        for chunk_num, chunk in enumerate(chunks, start=1):\n",
    "            all_chunks.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'text': chunk,\n",
    "                'file_name': page_data['file_name'],\n",
    "                'page_number': page_data['page_number'],\n",
    "                'chunk_number': chunk_num,\n",
    "                'char_count': len(chunk)\n",
    "            })\n",
    "            chunk_id += 1\n",
    "    \n",
    "    print(f\"âœ“ Created {len(all_chunks)} chunks from {len(pages_data)} pages\")\n",
    "    return all_chunks\n",
    "\n",
    "print(\"âœ“ PDF parsing and chunking functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample PDF for Testing\n",
    "\n",
    "Let's create a sample PDF with technical content for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Sample content created\n",
      "Content length: 4082 characters\n",
      "\n",
      "First 200 characters:\n",
      "Machine Learning Fundamentals\n",
      "\n",
      "Introduction to Machine Learning\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicit...\n"
     ]
    }
   ],
   "source": [
    "# Create sample content\n",
    "sample_content = \"\"\"Machine Learning Fundamentals\n",
    "\n",
    "Introduction to Machine Learning\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
    "\n",
    "Supervised Learning\n",
    "Supervised learning is a type of machine learning where the algorithm learns from labeled training data. The algorithm makes predictions based on input data and is corrected when its predictions are incorrect. Common applications include classification and regression tasks.\n",
    "\n",
    "Classification algorithms predict discrete labels. For example, determining whether an email is spam or not spam. Popular classification algorithms include logistic regression, decision trees, random forests, and support vector machines.\n",
    "\n",
    "Regression algorithms predict continuous values. For instance, predicting house prices based on features like size, location, and age. Linear regression and polynomial regression are fundamental regression techniques.\n",
    "\n",
    "Unsupervised Learning\n",
    "Unsupervised learning involves training algorithms on unlabeled data. The system tries to learn the patterns and structure from the data without explicit guidance. Clustering and dimensionality reduction are primary unsupervised learning techniques.\n",
    "\n",
    "K-means clustering groups similar data points together. It's widely used in customer segmentation, image compression, and anomaly detection. The algorithm iteratively assigns data points to clusters based on feature similarity.\n",
    "\n",
    "Deep Learning\n",
    "Deep learning uses artificial neural networks with multiple layers to progressively extract higher-level features from raw input. It has revolutionized fields like computer vision, natural language processing, and speech recognition.\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are particularly effective for image processing tasks. They use convolutional layers to automatically learn spatial hierarchies of features, making them ideal for tasks like image classification and object detection.\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed for sequence data. They maintain an internal state that captures information about previous inputs, making them suitable for tasks like language modeling and time series prediction.\n",
    "\n",
    "Natural Language Processing\n",
    "Natural Language Processing (NLP) focuses on the interaction between computers and human language. Modern NLP uses transformer architectures like BERT and GPT, which have achieved state-of-the-art results across various language tasks.\n",
    "\n",
    "Word embeddings represent words as dense vectors in a continuous vector space, where semantically similar words are closer together. Popular embedding techniques include Word2Vec, GloVe, and contextual embeddings from transformer models.\n",
    "\n",
    "Model Evaluation\n",
    "Evaluating machine learning models is crucial for understanding their performance. Common metrics include accuracy, precision, recall, F1-score for classification, and mean squared error, R-squared for regression.\n",
    "\n",
    "Cross-validation helps assess how well a model generalizes to unseen data. K-fold cross-validation divides the data into k subsets and trains the model k times, each time using a different subset for validation.\n",
    "\n",
    "Overfitting and Underfitting\n",
    "Overfitting occurs when a model learns the training data too well, including its noise, resulting in poor performance on new data. Regularization techniques like L1 and L2 regularization help prevent overfitting.\n",
    "\n",
    "Underfitting happens when a model is too simple to capture the underlying patterns in the data. This can be addressed by increasing model complexity or using more relevant features.\n",
    "\n",
    "Feature Engineering\n",
    "Feature engineering is the process of using domain knowledge to create features that make machine learning algorithms work better. Good features can significantly improve model performance.\n",
    "\n",
    "Feature scaling ensures that all features contribute equally to the model. Standardization and normalization are common scaling techniques that transform features to a similar scale.\n",
    "\"\"\"\n",
    "\n",
    "# Write to a text file (simulating PDF content)\n",
    "os.makedirs('sample_data', exist_ok=True)\n",
    "with open('sample_data/ml_fundamentals.txt', 'w') as f:\n",
    "    f.write(sample_content)\n",
    "\n",
    "print(\"âœ“ Sample content created\")\n",
    "print(f\"Content length: {len(sample_content)} characters\")\n",
    "print(f\"\\nFirst 200 characters:\\n{sample_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROCESSING SAMPLE DOCUMENT\n",
      "================================================================================\n",
      "\n",
      "âœ“ Extracted 18 sections\n",
      "{'page_number': 1, 'text': 'Machine Learning Fundamentals', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 2, 'text': 'Introduction to Machine Learning\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 3, 'text': 'Supervised Learning\\nSupervised learning is a type of machine learning where the algorithm learns from labeled training data. The algorithm makes predictions based on input data and is corrected when its predictions are incorrect. Common applications include classification and regression tasks.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 4, 'text': 'Classification algorithms predict discrete labels. For example, determining whether an email is spam or not spam. Popular classification algorithms include logistic regression, decision trees, random forests, and support vector machines.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 5, 'text': 'Regression algorithms predict continuous values. For instance, predicting house prices based on features like size, location, and age. Linear regression and polynomial regression are fundamental regression techniques.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 6, 'text': 'Unsupervised Learning\\nUnsupervised learning involves training algorithms on unlabeled data. The system tries to learn the patterns and structure from the data without explicit guidance. Clustering and dimensionality reduction are primary unsupervised learning techniques.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 7, 'text': \"K-means clustering groups similar data points together. It's widely used in customer segmentation, image compression, and anomaly detection. The algorithm iteratively assigns data points to clusters based on feature similarity.\", 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 8, 'text': 'Deep Learning\\nDeep learning uses artificial neural networks with multiple layers to progressively extract higher-level features from raw input. It has revolutionized fields like computer vision, natural language processing, and speech recognition.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 9, 'text': 'Convolutional Neural Networks (CNNs) are particularly effective for image processing tasks. They use convolutional layers to automatically learn spatial hierarchies of features, making them ideal for tasks like image classification and object detection.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 10, 'text': 'Recurrent Neural Networks (RNNs) are designed for sequence data. They maintain an internal state that captures information about previous inputs, making them suitable for tasks like language modeling and time series prediction.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 11, 'text': 'Natural Language Processing\\nNatural Language Processing (NLP) focuses on the interaction between computers and human language. Modern NLP uses transformer architectures like BERT and GPT, which have achieved state-of-the-art results across various language tasks.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 12, 'text': 'Word embeddings represent words as dense vectors in a continuous vector space, where semantically similar words are closer together. Popular embedding techniques include Word2Vec, GloVe, and contextual embeddings from transformer models.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 13, 'text': 'Model Evaluation\\nEvaluating machine learning models is crucial for understanding their performance. Common metrics include accuracy, precision, recall, F1-score for classification, and mean squared error, R-squared for regression.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 14, 'text': 'Cross-validation helps assess how well a model generalizes to unseen data. K-fold cross-validation divides the data into k subsets and trains the model k times, each time using a different subset for validation.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 15, 'text': 'Overfitting and Underfitting\\nOverfitting occurs when a model learns the training data too well, including its noise, resulting in poor performance on new data. Regularization techniques like L1 and L2 regularization help prevent overfitting.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 16, 'text': 'Underfitting happens when a model is too simple to capture the underlying patterns in the data. This can be addressed by increasing model complexity or using more relevant features.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 17, 'text': 'Feature Engineering\\nFeature engineering is the process of using domain knowledge to create features that make machine learning algorithms work better. Good features can significantly improve model performance.', 'file_name': 'ml_fundamentals.txt'}\n",
      "{'page_number': 18, 'text': 'Feature scaling ensures that all features contribute equally to the model. Standardization and normalization are common scaling techniques that transform features to a similar scale.\\n', 'file_name': 'ml_fundamentals.txt'}\n",
      "âœ“ Created 23 chunks\n",
      "\n",
      "================================================================================\n",
      "SAMPLE CHUNKS\n",
      "================================================================================\n",
      "\n",
      "Chunk 0:\n",
      "  File: ml_fundamentals.txt\n",
      "  Page: 1 | Chunk: 1\n",
      "  Length: 29 chars\n",
      "  Text: Machine Learning Fundamentals...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 1:\n",
      "  File: ml_fundamentals.txt\n",
      "  Page: 2 | Chunk: 1\n",
      "  Length: 291 chars\n",
      "  Text: Introduction to Machine Learning\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience wit...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "  File: ml_fundamentals.txt\n",
      "  Page: 2 | Chunk: 2\n",
      "  Length: 40 chars\n",
      "  Text: data and use it to learn for themselves....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# For this demo, we'll work with the text file\n",
    "# In practice, you would use actual PDF files\n",
    "\n",
    "def parse_text_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Parse text file as if it were a PDF.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split into \"pages\" by double newlines (paragraph breaks)\n",
    "    sections = content.split('\\n\\n')\n",
    "    \n",
    "    pages_data = []\n",
    "    for page_num, section in enumerate(sections, start=1):\n",
    "        if section.strip():\n",
    "            pages_data.append({\n",
    "                'page_number': page_num,\n",
    "                'text': section,\n",
    "                'file_name': Path(file_path).name\n",
    "            })\n",
    "    \n",
    "    return pages_data\n",
    "\n",
    "# Process the sample file\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCESSING SAMPLE DOCUMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "file_path = 'sample_data/ml_fundamentals.txt'\n",
    "pages_data = parse_text_file(file_path)\n",
    "\n",
    "print(f\"\\nâœ“ Extracted {len(pages_data)} sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks\n",
    "all_chunks = []\n",
    "chunk_id = 0\n",
    "\n",
    "for page_data in pages_data:\n",
    "    print(page_data)\n",
    "    chunks = chunk_text(page_data['text'], chunk_size=300, overlap=50)\n",
    "    \n",
    "    for chunk_num, chunk in enumerate(chunks, start=1):\n",
    "        all_chunks.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'text': chunk,\n",
    "            'file_name': page_data['file_name'],\n",
    "            'page_number': page_data['page_number'],\n",
    "            'chunk_number': chunk_num,\n",
    "            'char_count': len(chunk)\n",
    "        })\n",
    "        chunk_id += 1\n",
    "\n",
    "print(f\"âœ“ Created {len(all_chunks)} chunks\")\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE CHUNKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, chunk in enumerate(all_chunks[:3]):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  File: {chunk['file_name']}\")\n",
    "    print(f\"  Page: {chunk['page_number']} | Chunk: {chunk['chunk_number']}\")\n",
    "    print(f\"  Length: {chunk['char_count']} chars\")\n",
    "    print(f\"  Text: {chunk['text'][:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„ Parsing PDF: stephen_hawking_a_brief_history_of_time.pdf\n",
      "âœ“ Extracted text from 101 pages\n",
      "âœ“ Created 1599 chunks from 101 pages\n"
     ]
    }
   ],
   "source": [
    "all_chunks = process_pdf_to_chunks('stephen_hawking_a_brief_history_of_time.pdf', chunk_size=300, overlap=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Generate Embeddings\n",
    "\n",
    "Now we'll generate vector embeddings for each chunk using Sentence Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING EMBEDDINGS\n",
      "================================================================================\n",
      "\n",
      "Loading Sentence Transformer model...\n",
      "âœ“ Model loaded\n",
      "Embedding dimension: 384\n",
      "\n",
      "Generating embeddings for 1599 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:17<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Generated embeddings\n",
      "Embeddings shape: (1599, 384)\n",
      "  â€¢ 1599 chunks\n",
      "  â€¢ 384 dimensions per embedding\n",
      "\n",
      "âœ“ Embeddings added to chunk metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GENERATING EMBEDDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load embedding model\n",
    "print(\"\\nLoading Sentence Transformer model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"âœ“ Model loaded\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(f\"\\nGenerating embeddings for {len(all_chunks)} chunks...\")\n",
    "texts = [chunk['text'] for chunk in all_chunks]\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ“ Generated embeddings\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"  â€¢ {embeddings.shape[0]} chunks\")\n",
    "print(f\"  â€¢ {embeddings.shape[1]} dimensions per embedding\")\n",
    "\n",
    "# Add embeddings to chunks\n",
    "for chunk, embedding in zip(all_chunks, embeddings):\n",
    "    chunk['embedding'] = embedding\n",
    "\n",
    "print(\"\\nâœ“ Embeddings added to chunk metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Store in FAISS Vector Database\n",
    "\n",
    "FAISS is a library for efficient similarity search of high-dimensional vectors.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Index**: The data structure that stores vectors and enables fast search\n",
    "- **IndexFlatL2**: Exact search using L2 (Euclidean) distance\n",
    "- **IndexFlatIP**: Exact search using inner product (cosine similarity)\n",
    "- **IndexIVF**: Approximate search using inverted file indexes (faster, less accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING FAISS INDEX\n",
      "================================================================================\n",
      "\n",
      "Created FAISS IndexFlatL2 with dimension 384\n",
      "Index is trained: True\n",
      "Number of vectors: 0\n",
      "\n",
      "âœ“ Added 1599 vectors to FAISS index\n",
      "âœ“ Saved FAISS index to 'vector_dbs/faiss_index.bin'\n",
      "âœ“ Saved metadata to 'vector_dbs/faiss_metadata.json'\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CREATING FAISS INDEX\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create FAISS index\n",
    "# Using IndexFlatL2 for exact search with L2 distance\n",
    "dimension = embedding_dim\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "print(f\"\\nCreated FAISS IndexFlatL2 with dimension {dimension}\")\n",
    "print(f\"Index is trained: {faiss_index.is_trained}\")\n",
    "print(f\"Number of vectors: {faiss_index.ntotal}\")\n",
    "\n",
    "# Convert embeddings to float32 (FAISS requirement)\n",
    "embeddings_array = np.array([chunk['embedding'] for chunk in all_chunks]).astype('float32')\n",
    "\n",
    "# Add vectors to index\n",
    "faiss_index.add(embeddings_array)\n",
    "\n",
    "print(f\"\\nâœ“ Added {faiss_index.ntotal} vectors to FAISS index\")\n",
    "\n",
    "# Save index to disk\n",
    "os.makedirs('vector_dbs', exist_ok=True)\n",
    "faiss.write_index(faiss_index, 'vector_dbs/faiss_index.bin')\n",
    "print(\"âœ“ Saved FAISS index to 'vector_dbs/faiss_index.bin'\")\n",
    "\n",
    "# Save metadata separately (FAISS only stores vectors)\n",
    "metadata = []\n",
    "for chunk in all_chunks:\n",
    "    chunk_meta = chunk.copy()\n",
    "    chunk_meta.pop('embedding')  # Remove embedding array for JSON serialization\n",
    "    metadata.append(chunk_meta)\n",
    "\n",
    "with open('vector_dbs/faiss_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Saved metadata to 'vector_dbs/faiss_metadata.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Store in LanceDB Vector Database\n",
    "\n",
    "LanceDB is a modern vector database with built-in metadata support and SQL-like queries.\n",
    "\n",
    "### Key Features:\n",
    "- **Persistent**: Data is stored on disk\n",
    "- **Metadata**: Store vectors and metadata together\n",
    "- **SQL-like**: Familiar query syntax\n",
    "- **Versioning**: Track data changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING LANCEDB DATABASE\n",
      "================================================================================\n",
      "\n",
      "âœ“ Connected to LanceDB\n",
      "\n",
      "âœ“ Created table 'document_chunks' in LanceDB\n",
      "âœ“ Added 1599 records\n",
      "âœ“ Database saved to 'vector_dbs/lancedb'\n",
      "\n",
      "Table schema:\n",
      "  â€¢ chunk_id: integer\n",
      "  â€¢ text: string\n",
      "  â€¢ file_name: string\n",
      "  â€¢ page_number: integer\n",
      "  â€¢ chunk_number: integer\n",
      "  â€¢ char_count: integer\n",
      "  â€¢ vector: float array[384]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CREATING LANCEDB DATABASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Connect to LanceDB (creates database if it doesn't exist)\n",
    "lance_db = lancedb.connect('vector_dbs/lancedb')\n",
    "\n",
    "print(\"\\nâœ“ Connected to LanceDB\")\n",
    "\n",
    "# Prepare data for LanceDB\n",
    "# LanceDB expects a list of dictionaries with vector and metadata\n",
    "lance_data = []\n",
    "for chunk in all_chunks:\n",
    "    lance_data.append({\n",
    "        'chunk_id': chunk['chunk_id'],\n",
    "        'text': chunk['text'],\n",
    "        'file_name': chunk['file_name'],\n",
    "        'page_number': chunk['page_number'],\n",
    "        'chunk_number': chunk['chunk_number'],\n",
    "        'char_count': chunk['char_count'],\n",
    "        'vector': chunk['embedding'].tolist()  # Convert numpy array to list\n",
    "    })\n",
    "\n",
    "# Create table (or overwrite if exists)\n",
    "table_name = 'document_chunks'\n",
    "try:\n",
    "    # Drop table if it exists\n",
    "    lance_db.drop_table(table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new table\n",
    "table = lance_db.create_table(table_name, data=lance_data)\n",
    "\n",
    "print(f\"\\nâœ“ Created table '{table_name}' in LanceDB\")\n",
    "print(f\"âœ“ Added {len(lance_data)} records\")\n",
    "print(f\"âœ“ Database saved to 'vector_dbs/lancedb'\")\n",
    "\n",
    "# Display table info\n",
    "print(f\"\\nTable schema:\")\n",
    "print(f\"  â€¢ chunk_id: integer\")\n",
    "print(f\"  â€¢ text: string\")\n",
    "print(f\"  â€¢ file_name: string\")\n",
    "print(f\"  â€¢ page_number: integer\")\n",
    "print(f\"  â€¢ chunk_number: integer\")\n",
    "print(f\"  â€¢ char_count: integer\")\n",
    "print(f\"  â€¢ vector: float array[{embedding_dim}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Retrieval - Semantic Search\n",
    "\n",
    "Now let's implement semantic search using both FAISS and LanceDB.\n",
    "\n",
    "### How Semantic Search Works:\n",
    "1. Convert query text to embedding vector\n",
    "2. Find k-nearest neighbors in vector space\n",
    "3. Return chunks with highest similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Semantic search functions defined\n"
     ]
    }
   ],
   "source": [
    "def semantic_search_faiss(query: str, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform semantic search using FAISS.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with results and scores\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode([query])[0].astype('float32')\n",
    "    query_embedding = np.array([query_embedding])  # FAISS expects 2D array\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open('vector_dbs/faiss_metadata.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        result = metadata[idx].copy()\n",
    "        result['distance'] = float(distance)\n",
    "        result['similarity_score'] = 1 / (1 + distance)  # Convert distance to similarity\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def semantic_search_lancedb(query: str, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform semantic search using LanceDB.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with results and scores\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    # Open table\n",
    "    table = lance_db.open_table('document_chunks')\n",
    "    \n",
    "    # Search using vector similarity\n",
    "    results = table.search(query_embedding).limit(k).to_list()\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for result in results:\n",
    "        formatted_result = {\n",
    "            'chunk_id': result['chunk_id'],\n",
    "            'text': result['text'],\n",
    "            'file_name': result['file_name'],\n",
    "            'page_number': result['page_number'],\n",
    "            'chunk_number': result['chunk_number'],\n",
    "            'char_count': result['char_count'],\n",
    "            'distance': result['_distance'],\n",
    "            'similarity_score': 1 / (1 + result['_distance'])\n",
    "        }\n",
    "        formatted_results.append(formatted_result)\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "print(\"âœ“ Semantic search functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SEMANTIC SEARCH TEST\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: 'Fate of the Universe'\n",
      "================================================================================\n",
      "\n",
      "ðŸ” FAISS Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 0.545] Page 1, Chunk 2\n",
      "   t So Black\n",
      "Chapter 8 - The Origin and Fate of the Universe\n",
      "Chapter 9 - The Arrow of Time\n",
      "Chapter 10 - Wormholes and Time Travel\n",
      "Chapter 11 - The Unification of Physics\n",
      "Chapter 12 - Conclusion\n",
      "Glossary...\n",
      "\n",
      "2. [Score: 0.539] Page 71, Chunk 9\n",
      "   ofound implications for the role\n",
      "of God in the affairs of the universe. With the success of scientific theories in describing events, most people have\n",
      "come to believe that God allows the universe to e...\n",
      "\n",
      "3. [Score: 0.537] Page 94, Chunk 2\n",
      "   been an effective beginning of time. Similarly, if the whole universe recollapsed, there\n",
      "must be another state of infinite density in the future, the big crunch, which would be an end of time. Even if...\n",
      "\n",
      "\n",
      "ðŸ” LanceDB Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 0.545] Page 1, Chunk 2\n",
      "   t So Black\n",
      "Chapter 8 - The Origin and Fate of the Universe\n",
      "Chapter 9 - The Arrow of Time\n",
      "Chapter 10 - Wormholes and Time Travel\n",
      "Chapter 11 - The Unification of Physics\n",
      "Chapter 12 - Conclusion\n",
      "Glossary...\n",
      "\n",
      "2. [Score: 0.539] Page 71, Chunk 9\n",
      "   ofound implications for the role\n",
      "of God in the affairs of the universe. With the success of scientific theories in describing events, most people have\n",
      "come to believe that God allows the universe to e...\n",
      "\n",
      "3. [Score: 0.537] Page 94, Chunk 2\n",
      "   been an effective beginning of time. Similarly, if the whole universe recollapsed, there\n",
      "must be another state of infinite density in the future, the big crunch, which would be an end of time. Even if...\n",
      "\n",
      "================================================================================\n",
      "Query: 'accurate measurements'\n",
      "================================================================================\n",
      "\n",
      "ðŸ” FAISS Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 0.487] Page 20, Chunk 16\n",
      "   wed the errors were as\n",
      "great as the effect they were trying to measure. Their measurement had been sheer luck, or a case of knowing\n",
      "the result they wanted to get, not an uncommon occurrence in science...\n",
      "\n",
      "2. [Score: 0.468] Page 32, Chunk 1\n",
      "   measurement would be A in a certain number of cases, B in a different number, and so on. One could predict the\n",
      "approximate number of times that the result would be A or B, but one could not predict th...\n",
      "\n",
      "3. [Score: 0.454] Page 13, Chunk 3\n",
      "   we can measure time more\n",
      "accurately than length. In effect, the meter is defined to be the distance traveled by light in\n",
      "0.000000003335640952 second, as measured by a cesium clock. (The reason for tha...\n",
      "\n",
      "\n",
      "ðŸ” LanceDB Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 0.487] Page 20, Chunk 16\n",
      "   wed the errors were as\n",
      "great as the effect they were trying to measure. Their measurement had been sheer luck, or a case of knowing\n",
      "the result they wanted to get, not an uncommon occurrence in science...\n",
      "\n",
      "2. [Score: 0.468] Page 32, Chunk 1\n",
      "   measurement would be A in a certain number of cases, B in a different number, and so on. One could predict the\n",
      "approximate number of times that the result would be A or B, but one could not predict th...\n",
      "\n",
      "3. [Score: 0.454] Page 13, Chunk 3\n",
      "   we can measure time more\n",
      "accurately than length. In effect, the meter is defined to be the distance traveled by light in\n",
      "0.000000003335640952 second, as measured by a cesium clock. (The reason for tha...\n",
      "\n",
      "================================================================================\n",
      "Query: 'astronomy'\n",
      "================================================================================\n",
      "\n",
      "ðŸ” FAISS Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 0.544] Page 3, Chunk 1\n",
      "   CHAPTER 1\n",
      "OUR PICTURE OF THE UNIVERSE\n",
      "Â \n",
      "A well-known scientist (some say it was Bertrand Russell) once gave a public lecture on astronomy. He\n",
      "described how the earth orbits around the sun and how the ...\n",
      "\n",
      "2. [Score: 0.530] Page 5, Chunk 2\n",
      "   at year, Galileo started observing the night sky with a telescope, which had just been\n",
      "invented. When he looked at the planet Jupiter, Galileo found that it was accompanied by several small\n",
      "satellites...\n",
      "\n",
      "3. [Score: 0.521] Page 45, Chunk 17\n",
      "   , interest in the large-scale problems of astronomy and cosmology was\n",
      "revived by a great increase in the number and range of astronomical observations brought about by the application of\n",
      "modern techno...\n",
      "\n",
      "\n",
      "ðŸ” LanceDB Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 0.544] Page 3, Chunk 1\n",
      "   CHAPTER 1\n",
      "OUR PICTURE OF THE UNIVERSE\n",
      "Â \n",
      "A well-known scientist (some say it was Bertrand Russell) once gave a public lecture on astronomy. He\n",
      "described how the earth orbits around the sun and how the ...\n",
      "\n",
      "2. [Score: 0.530] Page 5, Chunk 2\n",
      "   at year, Galileo started observing the night sky with a telescope, which had just been\n",
      "invented. When he looked at the planet Jupiter, Galileo found that it was accompanied by several small\n",
      "satellites...\n",
      "\n",
      "3. [Score: 0.521] Page 45, Chunk 17\n",
      "   , interest in the large-scale problems of astronomy and cosmology was\n",
      "revived by a great increase in the number and range of astronomical observations brought about by the application of\n",
      "modern techno...\n"
     ]
    }
   ],
   "source": [
    "# Test semantic search\n",
    "print(\"=\" * 80)\n",
    "print(\"SEMANTIC SEARCH TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_queries = [\n",
    "    \"How do neural networks learn from data?\",\n",
    "    \"What is the difference between classification and regression?\",\n",
    "    \"Explain clustering algorithms\"\n",
    "]\n",
    "\n",
    "test_queries = [\n",
    "    \"Fate of the Universe\",\n",
    "    \"accurate measurements\",\n",
    "    \"astronomy\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Search with FAISS\n",
    "    print(\"\\nðŸ” FAISS Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    faiss_results = semantic_search_faiss(query, k=3)\n",
    "    \n",
    "    for i, result in enumerate(faiss_results, 1):\n",
    "        print(f\"\\n{i}. [Score: {result['similarity_score']:.3f}] \"\n",
    "              f\"Page {result['page_number']}, Chunk {result['chunk_number']}\")\n",
    "        print(f\"   {result['text'][:200]}...\")\n",
    "    \n",
    "    # Search with LanceDB\n",
    "    print(\"\\n\\nðŸ” LanceDB Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    lance_results = semantic_search_lancedb(query, k=3)\n",
    "    \n",
    "    for i, result in enumerate(lance_results, 1):\n",
    "        print(f\"\\n{i}. [Score: {result['similarity_score']:.3f}] \"\n",
    "              f\"Page {result['page_number']}, Chunk {result['chunk_number']}\")\n",
    "        print(f\"   {result['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Retrieval - Keyword Search\n",
    "\n",
    "Traditional keyword search looks for exact text matches.\n",
    "\n",
    "### Keyword Search vs Semantic Search:\n",
    "\n",
    "| Aspect | Keyword Search | Semantic Search |\n",
    "|--------|----------------|------------------|\n",
    "| Matching | Exact text match | Meaning-based |\n",
    "| Synonyms | Misses synonyms | Finds synonyms |\n",
    "| Context | No understanding | Context-aware |\n",
    "| Speed | Very fast | Fast (with index) |\n",
    "| Use Case | Known terms | Natural questions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Keyword search functions defined\n"
     ]
    }
   ],
   "source": [
    "def keyword_search(query: str, chunks: List[Dict], top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform simple keyword search.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        chunks: List of chunks to search\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of matching chunks with scores\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    query_terms = query_lower.split()\n",
    "    \n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        text_lower = chunk['text'].lower()\n",
    "        \n",
    "        # Count term matches\n",
    "        matches = sum(1 for term in query_terms if term in text_lower)\n",
    "        \n",
    "        if matches > 0:\n",
    "            score = matches / len(query_terms)  # Normalized score\n",
    "            result = chunk.copy()\n",
    "            result.pop('embedding', None)\n",
    "            result['keyword_score'] = score\n",
    "            result['matched_terms'] = matches\n",
    "            results.append(result)\n",
    "    \n",
    "    # Sort by score\n",
    "    results.sort(key=lambda x: x['keyword_score'], reverse=True)\n",
    "    \n",
    "    return results[:top_k]\n",
    "\n",
    "\n",
    "def keyword_search_lancedb(query: str, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform keyword search using LanceDB SQL-like queries.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of matching chunks\n",
    "    \"\"\"\n",
    "    table = lance_db.open_table('document_chunks')\n",
    "    \n",
    "    # Use SQL-like WHERE clause for text search\n",
    "    # Note: This is a simple contains search\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # LanceDB doesn't have full-text search built-in, so we'll get all and filter\n",
    "    all_results = table.to_pandas()\n",
    "    \n",
    "    # Filter based on keyword presence\n",
    "    query_terms = query_lower.split()\n",
    "    matches = []\n",
    "    \n",
    "    for _, row in all_results.iterrows():\n",
    "        text_lower = row['text'].lower()\n",
    "        matched_terms = sum(1 for term in query_terms if term in text_lower)\n",
    "        \n",
    "        if matched_terms > 0:\n",
    "            result = row.to_dict()\n",
    "            result.pop('vector', None)  # Remove vector for display\n",
    "            result['keyword_score'] = matched_terms / len(query_terms)\n",
    "            result['matched_terms'] = matched_terms\n",
    "            matches.append(result)\n",
    "    \n",
    "    # Sort by score\n",
    "    matches.sort(key=lambda x: x['keyword_score'], reverse=True)\n",
    "    \n",
    "    return matches[:k]\n",
    "\n",
    "print(\"âœ“ Keyword search functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KEYWORD SEARCH TEST\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: 'Fate of the Universe'\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Keyword Search Results (In-Memory):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 1.00, Matched: 4 terms] Page 1, Chunk 2\n",
      "   t So Black\n",
      "Chapter 8 - The Origin and Fate of the Universe\n",
      "Chapter 9 - The Arrow of Time\n",
      "Chapter 10 - Wormholes and Time Travel\n",
      "Chapter 11 - The Unification of Physics\n",
      "Chapter 12 - Conclusion\n",
      "Glossary...\n",
      "\n",
      "2. [Score: 1.00, Matched: 4 terms] Page 60, Chunk 19\n",
      "   swers that this\n",
      "approach suggests for the origin and fate of the universe and its contents, such as astronauts, will be de-scribed in the\n",
      "next two chapters. We shall see that although the uncertainty ...\n",
      "\n",
      "3. [Score: 1.00, Matched: 4 terms] Page 61, Chunk 1\n",
      "   CHAPTER 8\n",
      "THE ORIGIN AND FATE OF THE UNIVERSE\n",
      "Â \n",
      "Einsteinâ€™s general theory of relativity, on its own, predicted that space-time began at the big bang singularity and\n",
      "would come to an end either at the ...\n",
      "\n",
      "\n",
      "ðŸ”Ž Keyword Search Results (LanceDB):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 1.00, Matched: 4 terms] Page 1, Chunk 2\n",
      "   t So Black\n",
      "Chapter 8 - The Origin and Fate of the Universe\n",
      "Chapter 9 - The Arrow of Time\n",
      "Chapter 10 - Wormholes and Time Travel\n",
      "Chapter 11 - The Unification of Physics\n",
      "Chapter 12 - Conclusion\n",
      "Glossary...\n",
      "\n",
      "2. [Score: 1.00, Matched: 4 terms] Page 60, Chunk 19\n",
      "   swers that this\n",
      "approach suggests for the origin and fate of the universe and its contents, such as astronauts, will be de-scribed in the\n",
      "next two chapters. We shall see that although the uncertainty ...\n",
      "\n",
      "3. [Score: 1.00, Matched: 4 terms] Page 61, Chunk 1\n",
      "   CHAPTER 8\n",
      "THE ORIGIN AND FATE OF THE UNIVERSE\n",
      "Â \n",
      "Einsteinâ€™s general theory of relativity, on its own, predicted that space-time began at the big bang singularity and\n",
      "would come to an end either at the ...\n",
      "\n",
      "================================================================================\n",
      "Query: 'accurate measurements'\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Keyword Search Results (In-Memory):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 1.00, Matched: 2 terms] Page 15, Chunk 3\n",
      "   same whatever the\n",
      "speed of the source, and this has been confirmed by accurate measurements. It follows from this that if a pulse\n",
      "of light is emitted at a particular time at a particular point in spac...\n",
      "\n",
      "2. [Score: 0.50, Matched: 1 terms] Page 7, Chunk 11\n",
      "   theory is a good theory if it satisfies two requirements. It must accurately describe a large class of\n",
      "observations on the basis of a model that contains only a few arbitrary elements, and it must mak...\n",
      "\n",
      "3. [Score: 0.50, Matched: 1 terms] Page 7, Chunk 18\n",
      "   ed out the observation.\n",
      "In practice, what often happens is that a new theory is devised that is really an extension of the previous theory.\n",
      "For example, very accurate observations of the planet Mercur...\n",
      "\n",
      "\n",
      "ðŸ”Ž Keyword Search Results (LanceDB):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 1.00, Matched: 2 terms] Page 15, Chunk 3\n",
      "   same whatever the\n",
      "speed of the source, and this has been confirmed by accurate measurements. It follows from this that if a pulse\n",
      "of light is emitted at a particular time at a particular point in spac...\n",
      "\n",
      "2. [Score: 0.50, Matched: 1 terms] Page 7, Chunk 11\n",
      "   theory is a good theory if it satisfies two requirements. It must accurately describe a large class of\n",
      "observations on the basis of a model that contains only a few arbitrary elements, and it must mak...\n",
      "\n",
      "3. [Score: 0.50, Matched: 1 terms] Page 7, Chunk 18\n",
      "   ed out the observation.\n",
      "In practice, what often happens is that a new theory is devised that is really an extension of the previous theory.\n",
      "For example, very accurate observations of the planet Mercur...\n",
      "\n",
      "================================================================================\n",
      "Query: 'astronomy'\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Keyword Search Results (In-Memory):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 1.00, Matched: 1 terms] Page 3, Chunk 1\n",
      "   CHAPTER 1\n",
      "OUR PICTURE OF THE UNIVERSE\n",
      "Â \n",
      "A well-known scientist (some say it was Bertrand Russell) once gave a public lecture on astronomy. He\n",
      "described how the earth orbits around the sun and how the ...\n",
      "\n",
      "2. [Score: 1.00, Matched: 1 terms] Page 45, Chunk 13\n",
      "   ersuaded Chandrasekhar to abandon this line of work and turn instead to other problems in astronomy, such as the\n",
      "motion of star clusters. However, when he was awarded the Nobel Prize in 1983, it was, ...\n",
      "\n",
      "3. [Score: 1.00, Matched: 1 terms] Page 45, Chunk 17\n",
      "   , interest in the large-scale problems of astronomy and cosmology was\n",
      "revived by a great increase in the number and range of astronomical observations brought about by the application of\n",
      "modern techno...\n",
      "\n",
      "\n",
      "ðŸ”Ž Keyword Search Results (LanceDB):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [Score: 1.00, Matched: 1 terms] Page 3, Chunk 1\n",
      "   CHAPTER 1\n",
      "OUR PICTURE OF THE UNIVERSE\n",
      "Â \n",
      "A well-known scientist (some say it was Bertrand Russell) once gave a public lecture on astronomy. He\n",
      "described how the earth orbits around the sun and how the ...\n",
      "\n",
      "2. [Score: 1.00, Matched: 1 terms] Page 45, Chunk 13\n",
      "   ersuaded Chandrasekhar to abandon this line of work and turn instead to other problems in astronomy, such as the\n",
      "motion of star clusters. However, when he was awarded the Nobel Prize in 1983, it was, ...\n",
      "\n",
      "3. [Score: 1.00, Matched: 1 terms] Page 45, Chunk 17\n",
      "   , interest in the large-scale problems of astronomy and cosmology was\n",
      "revived by a great increase in the number and range of astronomical observations brought about by the application of\n",
      "modern techno...\n"
     ]
    }
   ],
   "source": [
    "# Test keyword search\n",
    "print(\"=\" * 80)\n",
    "print(\"KEYWORD SEARCH TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_keyword_queries = [\n",
    "    \"neural networks\",\n",
    "    \"classification regression\",\n",
    "    \"clustering\"\n",
    "]\n",
    "\n",
    "test_keyword_queries = [\n",
    "    \"Fate of the Universe\",\n",
    "    \"accurate measurements\",\n",
    "    \"astronomy\"\n",
    "]\n",
    "\n",
    "for query in test_keyword_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Keyword search (simple)\n",
    "    print(\"\\nðŸ”Ž Keyword Search Results (In-Memory):\")\n",
    "    print(\"-\" * 80)\n",
    "    keyword_results = keyword_search(query, all_chunks, top_k=3)\n",
    "    \n",
    "    if keyword_results:\n",
    "        for i, result in enumerate(keyword_results, 1):\n",
    "            print(f\"\\n{i}. [Score: {result['keyword_score']:.2f}, \"\n",
    "                  f\"Matched: {result['matched_terms']} terms] \"\n",
    "                  f\"Page {result['page_number']}, Chunk {result['chunk_number']}\")\n",
    "            print(f\"   {result['text'][:200]}...\")\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "    \n",
    "    # Keyword search with LanceDB\n",
    "    print(\"\\n\\nðŸ”Ž Keyword Search Results (LanceDB):\")\n",
    "    print(\"-\" * 80)\n",
    "    lance_keyword_results = keyword_search_lancedb(query, k=3)\n",
    "    \n",
    "    if lance_keyword_results:\n",
    "        for i, result in enumerate(lance_keyword_results, 1):\n",
    "            print(f\"\\n{i}. [Score: {result['keyword_score']:.2f}, \"\n",
    "                  f\"Matched: {result['matched_terms']} terms] \"\n",
    "                  f\"Page {result['page_number']}, Chunk {result['chunk_number']}\")\n",
    "            print(f\"   {result['text'][:200]}...\")\n",
    "    else:\n",
    "        print(\"No matches found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Comparison - Keyword vs Semantic Search\n",
    "\n",
    "Let's compare how the two search methods perform on the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KEYWORD VS SEMANTIC SEARCH COMPARISON\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: 'What techniques prevent overfitting?'\n",
      "Type: Query with no exact keyword matches\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž KEYWORD SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Score: 0.50\n",
      "   Overfitting and Underfitting\n",
      "Overfitting occurs when a model learns the training data too well, including its noise, resulting in poor performance on ...\n",
      "\n",
      "2. Score: 0.25\n",
      "   Regression algorithms predict continuous values. For instance, predicting house prices based on features like size, location, and age. Linear regressi...\n",
      "\n",
      "\n",
      "ðŸ” SEMANTIC SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Score: 0.582\n",
      "   Overfitting and Underfitting\n",
      "Overfitting occurs when a model learns the training data too well, including its noise, resulting in poor performance on ...\n",
      "\n",
      "2. Score: 0.528\n",
      "   Underfitting happens when a model is too simple to capture the underlying patterns in the data. This can be addressed by increasing model complexity o...\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š Analysis:\n",
      "  â€¢ Both methods found results\n",
      "  â€¢ Semantic search may find more relevant context\n",
      "\n",
      "================================================================================\n",
      "Query: 'neural networks deep learning'\n",
      "Type: Query with exact keywords\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž KEYWORD SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Score: 1.00\n",
      "   Deep Learning\n",
      "Deep learning uses artificial neural networks with multiple layers to progressively extract higher-level features from raw input. It has...\n",
      "\n",
      "2. Score: 0.50\n",
      "   Convolutional Neural Networks (CNNs) are particularly effective for image processing tasks. They use convolutional layers to automatically learn spati...\n",
      "\n",
      "\n",
      "ðŸ” SEMANTIC SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Score: 0.635\n",
      "   Deep Learning\n",
      "Deep learning uses artificial neural networks with multiple layers to progressively extract higher-level features from raw input. It has...\n",
      "\n",
      "2. Score: 0.501\n",
      "   Convolutional Neural Networks (CNNs) are particularly effective for image processing tasks. They use convolutional layers to automatically learn spati...\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š Analysis:\n",
      "  â€¢ Both methods found results\n",
      "  â€¢ Semantic search may find more relevant context\n",
      "\n",
      "================================================================================\n",
      "Query: 'How to evaluate model performance?'\n",
      "Type: Natural language question\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž KEYWORD SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Score: 0.60\n",
      "   Cross-validation helps assess how well a model generalizes to unseen data. K-fold cross-validation divides the data into k subsets and trains the mode...\n",
      "\n",
      "2. Score: 0.40\n",
      "   Word embeddings represent words as dense vectors in a continuous vector space, where semantically similar words are closer together. Popular embedding...\n",
      "\n",
      "\n",
      "ðŸ” SEMANTIC SEARCH:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Score: 0.564\n",
      "   Model Evaluation\n",
      "Evaluating machine learning models is crucial for understanding their performance. Common metrics include accuracy, precision, recall...\n",
      "\n",
      "2. Score: 0.415\n",
      "   include classification and regression tasks....\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š Analysis:\n",
      "  â€¢ Both methods found results\n",
      "  â€¢ Semantic search may find more relevant context\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KEYWORD VS SEMANTIC SEARCH COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_queries = [\n",
    "    (\"What techniques prevent overfitting?\", \"Query with no exact keyword matches\"),\n",
    "    (\"neural networks deep learning\", \"Query with exact keywords\"),\n",
    "    (\"How to evaluate model performance?\", \"Natural language question\")\n",
    "]\n",
    "\n",
    "for query, description in comparison_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Type: {description}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Keyword search\n",
    "    print(\"\\nðŸ”Ž KEYWORD SEARCH:\")\n",
    "    print(\"-\" * 80)\n",
    "    keyword_results = keyword_search(query, all_chunks, top_k=2)\n",
    "    \n",
    "    if keyword_results:\n",
    "        for i, result in enumerate(keyword_results, 1):\n",
    "            print(f\"\\n{i}. Score: {result['keyword_score']:.2f}\")\n",
    "            print(f\"   {result['text'][:150]}...\")\n",
    "    else:\n",
    "        print(\"âŒ No matches found\")\n",
    "    \n",
    "    # Semantic search\n",
    "    print(\"\\n\\nðŸ” SEMANTIC SEARCH:\")\n",
    "    print(\"-\" * 80)\n",
    "    semantic_results = semantic_search_faiss(query, k=2)\n",
    "    \n",
    "    for i, result in enumerate(semantic_results, 1):\n",
    "        print(f\"\\n{i}. Score: {result['similarity_score']:.3f}\")\n",
    "        print(f\"   {result['text'][:150]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š Analysis:\")\n",
    "    if not keyword_results:\n",
    "        print(\"  â€¢ Keyword search failed (no exact matches)\")\n",
    "        print(\"  â€¢ Semantic search succeeded (understood meaning)\")\n",
    "    elif len(keyword_results) < len(semantic_results):\n",
    "        print(\"  â€¢ Keyword search found fewer results\")\n",
    "        print(\"  â€¢ Semantic search more comprehensive\")\n",
    "    else:\n",
    "        print(\"  â€¢ Both methods found results\")\n",
    "        print(\"  â€¢ Semantic search may find more relevant context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Hybrid Search (Best of Both Worlds)\n",
    "\n",
    "Combine keyword and semantic search for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Hybrid search function defined\n"
     ]
    }
   ],
   "source": [
    "def hybrid_search(query: str, k: int = 5, keyword_weight: float = 0.3, \n",
    "                 semantic_weight: float = 0.7) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Combine keyword and semantic search with weighted scoring.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        k: Number of results\n",
    "        keyword_weight: Weight for keyword scores\n",
    "        semantic_weight: Weight for semantic scores\n",
    "    \n",
    "    Returns:\n",
    "        List of results with hybrid scores\n",
    "    \"\"\"\n",
    "    # Get both types of results\n",
    "    keyword_results = keyword_search(query, all_chunks, top_k=k*2)\n",
    "    semantic_results = semantic_search_faiss(query, k=k*2)\n",
    "    \n",
    "    # Create score dictionary\n",
    "    hybrid_scores = {}\n",
    "    \n",
    "    # Add keyword scores\n",
    "    for result in keyword_results:\n",
    "        chunk_id = result['chunk_id']\n",
    "        hybrid_scores[chunk_id] = {\n",
    "            'chunk': result,\n",
    "            'keyword_score': result['keyword_score'],\n",
    "            'semantic_score': 0.0\n",
    "        }\n",
    "    \n",
    "    # Add/update with semantic scores\n",
    "    for result in semantic_results:\n",
    "        chunk_id = result['chunk_id']\n",
    "        if chunk_id in hybrid_scores:\n",
    "            hybrid_scores[chunk_id]['semantic_score'] = result['similarity_score']\n",
    "        else:\n",
    "            hybrid_scores[chunk_id] = {\n",
    "                'chunk': result,\n",
    "                'keyword_score': 0.0,\n",
    "                'semantic_score': result['similarity_score']\n",
    "            }\n",
    "    \n",
    "    # Calculate hybrid scores\n",
    "    results = []\n",
    "    for chunk_id, scores in hybrid_scores.items():\n",
    "        hybrid_score = (keyword_weight * scores['keyword_score'] + \n",
    "                       semantic_weight * scores['semantic_score'])\n",
    "        \n",
    "        result = scores['chunk'].copy()\n",
    "        result['keyword_score'] = scores['keyword_score']\n",
    "        result['semantic_score'] = scores['semantic_score']\n",
    "        result['hybrid_score'] = hybrid_score\n",
    "        results.append(result)\n",
    "    \n",
    "    # Sort by hybrid score\n",
    "    results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "    \n",
    "    return results[:k]\n",
    "\n",
    "print(\"âœ“ Hybrid search function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HYBRID SEARCH TEST\n",
      "================================================================================\n",
      "\n",
      "Query: 'How does regularization prevent overfitting in neural networks?'\n",
      "================================================================================\n",
      "\n",
      "ðŸ”€ HYBRID SEARCH RESULTS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Hybrid Score: 0.618\n",
      "   â””â”€ Keyword: 0.500 | Semantic: 0.669\n",
      "   Page 15, Chunk 1\n",
      "   Overfitting and Underfitting\n",
      "Overfitting occurs when a model learns the training data too well, including its noise, resulting in poor performance on new data. Regularization techniques like L1 and L2...\n",
      "\n",
      "2. Hybrid Score: 0.389\n",
      "   â””â”€ Keyword: 0.250 | Semantic: 0.449\n",
      "   Page 8, Chunk 1\n",
      "   Deep Learning\n",
      "Deep learning uses artificial neural networks with multiple layers to progressively extract higher-level features from raw input. It has revolutionized fields like computer vision, natur...\n",
      "\n",
      "3. Hybrid Score: 0.364\n",
      "   â””â”€ Keyword: 0.250 | Semantic: 0.414\n",
      "   Page 9, Chunk 1\n",
      "   Convolutional Neural Networks (CNNs) are particularly effective for image processing tasks. They use convolutional layers to automatically learn spatial hierarchies of features, making them ideal for ...\n",
      "\n",
      "================================================================================\n",
      "âœ… Hybrid search combines:\n",
      "  â€¢ Keyword matching for exact term relevance\n",
      "  â€¢ Semantic understanding for context and meaning\n",
      "  â€¢ Weighted scoring for balanced results\n"
     ]
    }
   ],
   "source": [
    "# Test hybrid search\n",
    "print(\"=\" * 80)\n",
    "print(\"HYBRID SEARCH TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query = \"How does regularization prevent overfitting in neural networks?\"\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hybrid_results = hybrid_search(query, k=3)\n",
    "\n",
    "print(\"\\nðŸ”€ HYBRID SEARCH RESULTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"\\n{i}. Hybrid Score: {result['hybrid_score']:.3f}\")\n",
    "    print(f\"   â””â”€ Keyword: {result['keyword_score']:.3f} | \"\n",
    "          f\"Semantic: {result['semantic_score']:.3f}\")\n",
    "    print(f\"   Page {result['page_number']}, Chunk {result['chunk_number']}\")\n",
    "    print(f\"   {result['text'][:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Hybrid search combines:\")\n",
    "print(\"  â€¢ Keyword matching for exact term relevance\")\n",
    "print(\"  â€¢ Semantic understanding for context and meaning\")\n",
    "print(\"  â€¢ Weighted scoring for balanced results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Loading Saved Indexes\n",
    "\n",
    "Demonstrate how to load previously saved vector databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING SAVED VECTOR DATABASES\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‚ Loading FAISS index...\n",
      "âœ“ Loaded FAISS index with 23 vectors\n",
      "âœ“ Loaded 23 metadata records\n",
      "\n",
      "ðŸ“‚ Loading LanceDB...\n",
      "âœ“ Loaded LanceDB with 23 records\n",
      "\n",
      "ðŸ” Testing search with loaded indexes...\n",
      "Query: 'machine learning'\n",
      "\n",
      "FAISS Results:\n",
      "  â€¢ Score: 0.676 - Machine Learning Fundamentals...\n",
      "  â€¢ Score: 0.563 - Introduction to Machine Learning\n",
      "Machine learning is a subset of artificial intelligence that enable...\n",
      "\n",
      "LanceDB Results:\n",
      "  â€¢ Score: 0.676 - Machine Learning Fundamentals...\n",
      "  â€¢ Score: 0.563 - Introduction to Machine Learning\n",
      "Machine learning is a subset of artificial intelligence that enable...\n",
      "\n",
      "âœ… Successfully loaded and searched both vector databases!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOADING SAVED VECTOR DATABASES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load FAISS index\n",
    "print(\"\\nðŸ“‚ Loading FAISS index...\")\n",
    "loaded_faiss_index = faiss.read_index('vector_dbs/faiss_index.bin')\n",
    "print(f\"âœ“ Loaded FAISS index with {loaded_faiss_index.ntotal} vectors\")\n",
    "\n",
    "# Load FAISS metadata\n",
    "with open('vector_dbs/faiss_metadata.json', 'r') as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "print(f\"âœ“ Loaded {len(loaded_metadata)} metadata records\")\n",
    "\n",
    "# Connect to LanceDB\n",
    "print(\"\\nðŸ“‚ Loading LanceDB...\")\n",
    "loaded_lance_db = lancedb.connect('vector_dbs/lancedb')\n",
    "loaded_table = loaded_lance_db.open_table('document_chunks')\n",
    "record_count = loaded_table.count_rows()\n",
    "print(f\"âœ“ Loaded LanceDB with {record_count} records\")\n",
    "\n",
    "# Test search with loaded indexes\n",
    "print(\"\\nðŸ” Testing search with loaded indexes...\")\n",
    "test_query = \"machine learning\"\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "\n",
    "# Search loaded FAISS\n",
    "query_embedding = embedding_model.encode([test_query])[0].astype('float32')\n",
    "query_embedding = np.array([query_embedding])\n",
    "distances, indices = loaded_faiss_index.search(query_embedding, 2)\n",
    "\n",
    "print(\"FAISS Results:\")\n",
    "for idx, distance in zip(indices[0], distances[0]):\n",
    "    print(f\"  â€¢ Score: {1/(1+distance):.3f} - {loaded_metadata[idx]['text'][:100]}...\")\n",
    "\n",
    "# Search loaded LanceDB\n",
    "query_embedding = embedding_model.encode([test_query])[0]\n",
    "lance_results = loaded_table.search(query_embedding).limit(2).to_list()\n",
    "\n",
    "print(\"\\nLanceDB Results:\")\n",
    "for result in lance_results:\n",
    "    print(f\"  â€¢ Score: {1/(1+result['_distance']):.3f} - {result['text'][:100]}...\")\n",
    "\n",
    "print(\"\\nâœ… Successfully loaded and searched both vector databases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### ðŸŽ¯ What We Learned\n",
    "\n",
    "1. **Vector Databases** enable efficient similarity search at scale\n",
    "2. **FAISS** provides fast in-memory vector search\n",
    "3. **LanceDB** offers persistent storage with metadata support\n",
    "\n",
    "### ðŸ“Š Comparison: FAISS vs LanceDB\n",
    "\n",
    "| Feature | FAISS | LanceDB |\n",
    "|---------|-------|----------|\n",
    "| **Storage** | In-memory (with save/load) | Disk-based, persistent |\n",
    "| **Metadata** | Separate storage required | Built-in support |\n",
    "| **Speed** | Extremely fast | Fast |\n",
    "| **Scalability** | Limited by RAM | Scales to disk |\n",
    "| **Queries** | Vector search only | Vector + SQL-like |\n",
    "| **Use Case** | Research, prototypes | Production apps |\n",
    "| **Updates** | Rebuild index | Easy updates |\n",
    "| **Versioning** | Manual | Built-in |\n",
    "\n",
    "### ðŸ” Search Methods Comparison\n",
    "\n",
    "| Method | Pros | Cons | Best For |\n",
    "|--------|------|------|----------|\n",
    "| **Keyword** | Fast, exact matches | Misses synonyms, no context | Known terms, filters |\n",
    "| **Semantic** | Understands meaning | Slower, needs embeddings | Natural questions |\n",
    "| **Hybrid** | Best of both worlds | More complex | Production systems |\n",
    "\n",
    "### ðŸš€ Production Considerations\n",
    "\n",
    "1. **Chunking Strategy**: Balance context vs specificity (300-500 chars)\n",
    "2. **Overlap**: Use 10-20% overlap to maintain context\n",
    "3. **Metadata**: Store page, chunk, file info for citations\n",
    "4. **Index Type**: Choose based on scale and accuracy needs\n",
    "5. **Hybrid Search**: Combine methods for best results\n",
    "\n",
    "### ðŸ› ï¸ Next Steps\n",
    "\n",
    "- Implement RAG pipeline with vector database\n",
    "- Add filtering by metadata\n",
    "- Experiment with different chunking strategies\n",
    "- Try other vector databases (Chroma, Pinecone, Weaviate)\n",
    "- Implement re-ranking for better results\n",
    "\n",
    "### ðŸ“š Additional Vector Databases\n",
    "\n",
    "- **Chroma**: Simple, embedded, great for prototypes\n",
    "- **Pinecone**: Managed, cloud-native, production-ready\n",
    "- **Weaviate**: GraphQL API, hybrid search built-in\n",
    "- **Qdrant**: Rust-based, high performance\n",
    "- **Milvus**: Large-scale, distributed\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Conclusion\n",
    "\n",
    "Vector databases are the foundation of modern AI applications. They enable:\n",
    "- **Fast semantic search** across millions of documents\n",
    "- **Efficient RAG** pipelines for LLMs\n",
    "- **Scalable** similarity search\n",
    "- **Flexible** metadata filtering\n",
    "\n",
    "Choose the right vector database for your use case:\n",
    "- **Prototype/Research**: FAISS, Chroma\n",
    "- **Production**: LanceDB, Pinecone, Weaviate\n",
    "- **Large Scale**: Milvus, Qdrant\n",
    "\n",
    "**The combination of embeddings + vector databases powers the next generation of AI applications!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
